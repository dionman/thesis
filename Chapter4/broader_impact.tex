\section{Broader Impact}
\label{sec:broader_impact}

Pseudocoreset variational inference is a general-purpose Bayesian inference
algorithm, hence shares implications mostly encountered in approximate
inference methods. For example, replacing the full dataset with a
pseudocoreset has the potential to cause inferential errors; these can be
partially tempered by using a pseudocoreset of larger size. Note also
that the optimization algorithm in this work aims to reduce 
KL divergence: however the proposed
variational objective might be misleading in many applications and lead to
incorrect conclusions in certain statistical models (e.g. point estimates and
uncertainties might be far off despite KL being almost zero~\citep{huggins20}).
 Moreover, Bayesian inference in general is prone to model misspecification.
Therefore, a pseudocoreset summarization based on a wrong statistical model
will lead to non-representative compression for inferential purposes.
Constructing the coreset on a statistical model suited for robust inference
instead of the original one~\citep{miller19, wang17}, can offer protection
against modeling mismatches. Naturally, the utility of generated dataset
summary becomes task-dependent, as it has been optimized for a specific
learning objective, and cannot be fully transferable to multiple different
inference tasks on the same dataset.

Our learnable pseudodata are also generally not as interpretable 
as the points of previous coreset methods, as they are not real data. And the level of 
interpretability is model specific. This creates a risk of misinterpretation
of pseudocoreset points in practice. On the other hand, our optimization framework
does allow the introduction of interpretability constraints (e.g.~pseudodata sparsity)
to explicitly capture interpretability requirements.

Pseudocoreset-based summarization is susceptible to reproducing potential
biases and unfairness existing in the original dataset. Majority-group datapoints in the full dataset which capture information relevant to the
statistical task of interest are expected to remain over-represented in the
learned summary; while minority-group datapoints might be eliminated, if their
distinguishing features are not related to inference. Amending the
initialization step to contain such datapoints or using a prior that
strongly favors a debiased version of the dataset could both mitigate these
concerns; but more study is warranted.

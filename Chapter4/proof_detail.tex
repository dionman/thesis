\section{Technical Results and Proofs}
\label{supp:proofs}

\subsection{Proof of~\cref{prop:original_coreset_fails}}
In the setting of~\cref{prop:original_coreset_fails}, both the exact posterior and the coreset posterior 
are multivariate Gaussian distributions, denoted as $\distNorm{\left(\mu_1, \Sigma_1\right)}$ and 
$\distNorm{\left(\mu_{w}, \Sigma_{w}\right)}$ respectively. 
The mean and covariance are
\[
\Sigma_{1}=\frac{1}{1+ N} I_d, \quad \mu_{1}=\Sigma_{1}\left( \sum_{n=1}^{N} X_{n}\right), 
\label{eq:exact_post}
\]
and
\[
\label{eq:coreset_post}
\hspace{-.3cm}\Sigma_{w}\!=\!\frac{I_d}{1+ \left(\sum_{n=1}^N w_n\right)}, 
\quad
\mu_{w}\!=\!\Sigma_{w}\left( \sum_{n=1}^{N} w_n X_{n}\right).
\]

\begin{proof}[Proof of~\cref{prop:original_coreset_fails}]
By~\cref{eq:exact_post,eq:coreset_post}, 
\begin{equation} \label{eq:KL_from_coreset_post_to_exact}
\begin{aligned}
\kl{\pi_{w}}{\pi_1}
 =& \frac{1}{2}\left[\log\frac{|\Sigma_{1}|}{|\Sigma_{w}|} - d + \tr\left( \Sigma_{1}^{-1}\Sigma_{w}\right)   
  (\mu_{1} - \mu_{w})^T \Sigma_{1}^{-1}(\mu_{1} - \mu_{w})\right]\\
=& \frac{1}{2} \left[ -d\log \left( \frac{1+ N}{1+ \sum_{n=1}^N w_n}\right) - d  + d \left( \frac{1+ N}{1+ \sum_{n=1}^N w_n}\right)
+  (\mu_{1} - \mu_{w})^T \Sigma_{1}^{-1}(\mu_{1} - \mu_{w})\right].
\end{aligned}
\end{equation}
Note that $\forall x > 0, x-1 \geq \log x$, implying that 
$$ d\log \left( \frac{1+ N}{1+ \sum_{n=1}^N w_n}\right) -d + d \left( \frac{1+ N}{1+ \sum_{n=1}^N w_n}\right) > 0.$$ 
Thus, 
\begin{equation} \label{eq:first_inequality}
\kl{\pi_{w}}{\pi_1} \geq \frac{1}{2}(\mu_{1} - \mu_{w})^T \Sigma_{1}^{-1}(\mu_{1} - \mu_{w}).
\end{equation}

Suppose we pick a set $\mcI\subseteq[N]$, $\left|\mcI\right| = M$ of active indices $n$ where the optimal $w_n \geq 0$,
 and enforce that all others $n\notin \mcI$ satisfy $w_n = 0$.
Then denoting
\[
Y = \left[X_n : n\notin \mcI\right] \in \reals^{d\times (N-M)}, \quad
X = \left[X_n : n\in \mcI\right] \in \reals^{d\times M},
\]
we have that for any $w\in \reals_+^M$ for those indices $\mcI$,
\[
\kl{\pi_w}{\pi_1} 
\geq & \frac{1}{2(N+1)}1^TY^TY1 
+1^TY^TX\left(\frac{1}{N+1} - \frac{w}{1+1^Tw}\right)\\
&+\frac{N+1}{2}\!\!\left(\frac{1}{N+1}\! -\! \frac{w}{1+1^Tw}\right)^T\!\!\!\!\!X^T\!X\!\left(\frac{1}{N+1} \!-\! \frac{w}{1+1^Tw}\right).
\]
Relaxing the nonnegativity constraint, replacing $w/(1+1^Tw)$ with a generic $x\in\reals^M$, and 
noting that $X^TX$ is invertible almost surely when $M < d$,
we can optimize this expression yielding a lower bound
on the optimal KL divergence using active index set $\mcI$,
\[
\kl{\pi_{w^\star_{\mcI}}}{\pi_1} &\geq \frac{1^TY^T\left(I-X(X^TX)^{-1}X^T\right)Y1}{2(N+1)}.
\]
The numerator is the squared norm of $Y1$ minus its projection onto the subspace spanned by the $M$ columns of $X$.
Since $Y1 \dist \distNorm(0, (N-M)I)$, $Y1 \in \reals^d$ is an isotropic Gaussian, then its projection into the orthogonal
complement of any fixed subspace of dimension $M$ is also an isotropic Gaussian of dimension $d-M$ with the same variance.
Since the columns of $X$ are also independent and isotropic, its column subspace is uniformly distributed.
So therefore, for each possible choice of $\mcI$
\[
\kl{\pi_{w^\star_{\mcI}}}{\pi_1} &\geq \frac{N-M}{2(N+1)} Z_{\mcI},  \quad Z_{\mcI}\dist \chi^2(d-M).
\]
Note that the $Z_\mcI$ will have dependence across the $N\choose M$ different choices of index subset $\mcI$.
Thus, the probability that \emph{all} $Z_{\mcI}$ are large is
\[
\Pr\left(\min_{\mcI \subseteq [N], |\mcI| = M} Z_{\mcI} > \epsilon\right) 
\geq &1 - {N\choose M}\Pr\left(Z_{\mcI} \leq \epsilon\right)\\
= &1 - {N\choose M}F_{d-M}(\epsilon),
\]
where $F_{k}$ is the CDF for the $\chi^2$ distribution with $k$ degrees of freedom.
The result follows.
\end{proof}



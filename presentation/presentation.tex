\documentclass[hyperref={colorlinks = true},unknownkeysallowed]{beamer}
\usepackage{hyperref}%
\usepackage{graphicx} % graphics
\usepackage{epsfig} % eps graphics
\usepackage{booktabs, caption} % table styling
\usepackage{pgfplots}
\usepackage{amsmath}
\usepackage{tikz}
\usetikzlibrary{positioning}
\usetikzlibrary{fit}
\usetikzlibrary{backgrounds}
\usetikzlibrary{calc}
\usetikzlibrary{shapes}
\usetikzlibrary{mindmap}
\usetikzlibrary{decorations.text}
\usetikzlibrary{arrows.meta,arrows}
\usetikzlibrary{shapes.geometric}
\usepackage{changepage}
\usepackage{color}
\usepackage{caption}
\usepackage{transparent}
\usepackage{fontspec}
\usepackage{tcolorbox}
\usetikzlibrary{shapes}
\usepackage{natbib}
\bibliographystyle{plainnat}
\pgfplotsset{compat=1.7}

\makeatletter
\def\blfootnote{\xdef\@thefnmark{}\@footnotetext}
\makeatother

\definecolor{UBCblue}{rgb}{0.04706, 0.13725, 0.26667} % UBC Blue (primary)
\usecolortheme[named=UBCblue]{structure}

\let\oldbibitem=\bibitem
\renewcommand{\bibitem}[2][]{\label{#2}\oldbibitem[#1]{#2}}
\let\oldcite=\cite
\renewcommand\cite[1]{\hypersetup{linkcolor=UBCblue} \hyperlink{#1}{\oldcite{#1}}}
\let\oldcitep=\citep
\renewcommand\citep[1]{\hypersetup{linkcolor=UBCblue}\hyperlink{#1}{\oldcitep{#1}}}
\let\oldciteauthor=\citeauthor
\renewcommand\citeauthor[1]{\hypersetup{linkcolor=UBCblue}\hyperlink{#1}{\oldciteauthor{#1}}} 


% suppress navigation bar
\beamertemplatenavigationsymbolsempty

\setbeamercolor{normal text}{fg=UBCblue}
\setbeamercolor{frametitle}{bg=white, fg=UBCblue}

\setbeamercolor{section title}{fg=UBCblue, bg=white}
\setbeamercolor{headline}{bg=white, fg=UBCblue}
\setbeamercolor*{palette primary}{fg=UBCblue,bg=UBCblue}
\setbeamercolor*{palette secondary}{fg=UBCblue,bg=UBCblue}
\setbeamercolor*{palette tertiary}{fg=UBCblue,bg=UBCblue}

\setbeamertemplate{itemize subitem}[triangle]


% see the macros.tex file for definitions
\include{macros}

% command shortcuts
\newcommand{\tikzmark}[1]{\tikz[overlay,remember picture] \node (#1) {};}
\tikzset{
	block/.style={rectangle, draw, fill=white!40, text width=13em,
		text centered, rounded corners, minimum height=2em},
}
\newcommand{\thinkB}[1] {
	\begin{tikzpicture}
	\node [rectangle, draw, decoration=bumps, decorate, align=left, inner sep=4mm] {#1};
	\end{tikzpicture}
}

\newcommand{\thinkA}[1] {
	\begin{tikzpicture}
	\node[cloud, draw, align=left, cloud puffs=20,cloud puff arc=110, aspect=2, minimum width=.3cm, minimum height=.3cm, inner sep=0pt,
	text width=6.5cm, inner sep=0mm]{#1};
	\end{tikzpicture}
}

\definecolor{darkred}{rgb}{0.55, 0.0, 0.0}


% for resuming lists across frames
\newcounter{savedenum}
\newcommand*{\saveenum}{\setcounter{savedenum}{\theenumi}}
\newcommand*{\resume}{\setcounter{enumi}{\thesavedenum}}

\setbeamertemplate{bibliography entry article}{}
\setbeamertemplate{bibliography entry title}{}
\setbeamertemplate{bibliography entry location}{}
\setbeamertemplate{bibliography entry note}{}


% title slide definition
\title{Data Summarizations for Scalable, Robust and Privacy-Aware Learning in High-Dimensions}
\author{Dionysis Manousakas}
\institute{Department of Computer Science \& Technology, \\University of Cambridge}
\date{}

%--------------------------------------------------------------------
%                           Introduction
%--------------------------------------------------------------------

\begin{document}
	
\begin{frame}
\vspace{2cm}
  \titlepage
  \vspace{10cm}
\end{frame}

\begin{frame}{Structure}
	\begin{itemize}
		\item[I.]\textbf{Quantifying Privacy Loss of Human Mobility Graph Topology}  \small{\citep{manousakas2018quantifying}} \\
		An empirical study on longitudinal mobility data, highlighting overlooked privacy threats on reduced data representations
		\item[II.]\textbf{Bayesian Pseudocoresets} \small{\citep{psvi}} \\
		A variational framework for scalable Bayesian inference, that achieves superior summarization quality in high-dimensions and admits differentially private constructions
		\item[III.]\textbf{$\beta$-Cores} \small{\citep{beta-cores}}  \\
		A framework for Bayesian summarization designed for observations following the Huber contamination model.
	\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% PETS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}
	\LARGE{\textbf{I. Quantifying Privacy Loss of Human Mobility Graph Topology}}
\end{frame}

\begin{frame}{ Mobility data privacy vs. utility}
	\vspace*{2.25mm}
\begin{block}{ $\bullet$ Information sharing for data-driven customization and large-scale analytics}
	\begin{itemize}
		  \setlength\itemsep{.3em}
		\item --- context-awareness
		\item --- transportation management, health studies, urban development
	\end{itemize}
\end{block}
\pause
\begin{block}{ $\bullet$ \textbf{Utility}-preserving anonymized data representations}
	\begin{itemize}
		  \setlength\itemsep{.3em}
	%\item timestamped GPS measurements
	\item --- histograms, heatmaps, \textbf{graphs}
	\end{itemize}
\end{block}
\pause
\begin{block}{ $\bullet$ How \textbf{privacy} conscientious they are?}
	\begin{itemize}
		\item --- often poorly understood, leading to privacy breaches
	\end{itemize}
\end{block}
\end{frame}


\begin{frame}{Deanonymizing mobility}
	%\includegraphics[width=5cm,height=3.5cm]{figs/rawtraces.png}
	\begin{columns}
		\begin{column}[t]{0.5\textwidth}
	\begin{block}{Inference on \textbf{individual}  traces }
	\circled{\textbf{1}} Sparsity and regularity-based \\
	\begin{itemize}
		\item --- "top-$N$" location attacks {\footnotesize \citep{Zang2011}}
		\item --- unicity of spatio-temporal points {\footnotesize \citep{DeMontjoye2013}}
		\item --- matching of individual mobility histograms {\footnotesize \citep{Naini2016a}}
	\end{itemize}
	\circled{\textbf{2}} Probabilistic models \\
\begin{itemize}
	\item --- Markovian mobility models {\footnotesize \citep{deMulder08}}
	\item --- Mobility Markov chains {\footnotesize \citep{Gambs2014}}
\end{itemize}
\end{block}			
\end{column}
\pause
\begin{column}[t]{0.5\textwidth}
\begin{block}{Inference on \textbf{population} statistics}
	\circled{\textbf{3}} On aggregate information\\
	\begin{itemize}
		\item --- Individual trajectory recovery from aggregated mobility data {\footnotesize \citep{xu2017trajectory}}
		\item --- Probabilistic inference on aggregated location time-series {\footnotesize \citep{pyrgelis2017does}}
	\end{itemize}
\end{block}
		\end{column}
	\end{columns}
\end{frame}



\begin{frame}{Mobility representations}
	\begin{columns}
		\begin{column}[t]{0.33\textwidth}
			\begin{figure}
				\includegraphics[width=3cm,height=7cm]{figs/mobility_representations.png}
			\end{figure}
		\end{column}
		\begin{column}[t]{0.66\textwidth}
			\begin{tikzpicture}[baseline=(current bounding box.north)]
			\node at (2.7,-1.2) (nodeA) {
					\begin{tabular}{cc}
				     \textbf{raw mobility} \\
					\textbf{data} \\
					\end{tabular}
				};
			\pause
			\node at (2.7,-7.05) (nodeB) {
				\begin{tabular}{cc}
				\textbf{sequences of} \\
				 \textbf{pseudonymised} \\
				 \textbf{regions of interest} \\
				 \footnotesize{e.g. MDC research track,} \\ \footnotesize{Device Analyzer}
				\end{tabular}
			};
			\node at (4.3,-1) (nodeA00) {};
			\node at (4.3,-7) (nodeB00) {};
			\node at (5.5,-1) (nodeA0) {};
			\node at (5.5,-7) (nodeB0) {};
			\node at (6.7,-1) (nodeA1) {};
			\node at (6.7,-7) (nodeB1) {};
			\node at (7.9,-1) (nodeA2) {};
			\node at (7.9,-7) (nodeB2) {};
								\pause
			\draw [line width=0.925mm,<-] (nodeA00) -- (nodeB00) node [midway, above, sloped] (TextNode) {storage cost};
					\pause
			\draw [line width=0.925mm,<-] (nodeA0) -- (nodeB0) node [midway, above, sloped] (TextNode) {utility};
					\pause
			\draw [line width=0.925mm,->] (nodeA1) -- (nodeB1) node [midway, above, sloped] (TextNode) {inference difficulty};
					\pause
			\draw [line width=0.925mm,--, red!95!black!50](nodeA2) -- (nodeB2) node [midway, above, sloped] (TextNode) {\textcolor{red!95!black!50}{privacy loss ?} };
			\end{tikzpicture}
		\end{column}
	\end{columns}
\end{frame}


\begin{frame}{Motivation}
	\begin{columns}
		\begin{column}[t]{0.35\textwidth}
			\begin{center}
			\centering	\includegraphics[width=4cm,height=3cm]{figs/mobility_network.png}\\[.5\baselineskip]
		\end{center}
	\begin{center}
	\begin{tikzpicture}
	\visible<1>{\node[opacity=0.3] (img2) {\includegraphics[width=1.7cm, height=1.7cm]{inspector.png}};}
	\visible<2>{\node[opacity=0.9] (img2) {\includegraphics[width=1.7cm, height=1.7cm]{inspector.png}};}
	\end{tikzpicture}
\end{center}
		\end{column}
		\begin{column}[t]{0.65\textwidth}
			\begin{block}{Let's remove}
				\begin{itemize}
					\item --- temporal (except from \emph{ordering} of states)
					\item --- geographic, and
					\item --- cross-referencing information
				\end{itemize}
			\end{block}
			\pause
			\begin{block}{$\blacktriangleright$ What is the privacy leakage of this representation? \\ 
									$\blacktriangleright$ Does \emph{topology} still bear identifiable information? \\
									$\blacktriangleright$ Can an adversary exploit it in a deanonymization attack? }
			\end{block}
		\end{column}
	\end{columns}
\end{frame}


\begin{frame}[label=workflow]{Mobility information flow}
	\makebox[\textwidth][c]{%
		\begin{tikzpicture}[
		outpt/.style={->,black!80!black,very thick,-latex'},
		>=stealth,
		every node/.append style={align=center}]
		\node (kaela) {\begin{tabular}{@{}c}Removal of\\Geographic-Temporal\\Information  \end{tabular}};
		\node (source) [above = of kaela,draw=black!50,dashed,circle,fill=orange!30]{Mobility \\ Data};
		\draw[outpt](source)--(kaela);
		\node (accessfile) [right=of kaela] {\begin{tabular}{@{}c} Graph\\Topology \end{tabular}};
		\draw[outpt](kaela)--(accessfile);
		% Draw background
		\begin{pgfonlayer}{background}
		% Left-top corner of the background rectangle
		\path (kaela.west |- kaela.north)+(-0.5,0.5) node (a) {};
		% Right-bottom corner of the background rectanle
		\path (accessfile.east |- accessfile.south)+(+0.5,-0.5) node (c) {};
		% Draw the background
		\path[fill=yellow!1,rounded corners, draw=black!50, dashed]
		(a) rectangle (c);
		\end{pgfonlayer}
		\begin{pgfonlayer}{background}
		% Left-top corner of the background rectangle
		\path (kaela.west |- kaela.north)+(-0.5,0.5) node (a) {};
		% Right-bottom corner of the background rectanle
		\path (accessfile.east |- accessfile.south)+(+0.5,-0.5) node (c) {};
		% Draw the background
		\path[fill=yellow!30,rounded corners, draw=black!50, dashed]
		(a) rectangle (c);
		\end{pgfonlayer}
		\node (screen)[above right=of accessfile]{Sparsity};
		\node (braille)[below right =of accessfile]{Recurrence};
		\coordinate (middle) at ($(screen.east)!0.5!(braille.east)$);
		\draw[outpt](accessfile)--(screen.west);
		\draw[outpt](accessfile)--(braille);
		\begin{pgfonlayer}{background}
		% Left-top corner of the background rectangle
		\path (screen.west |- screen.north)+(-0.15,0.15) node (a) {};
		% Right-bottom corner of the background rectanle
		\path (braille.east |- braille.south)+(0.25,-0.25) node (b) {};
		% Draw the background
		\path[fill=green!40,rounded corners, draw=green,thick]
		(a) rectangle (b);
		\end{pgfonlayer}
		\node (enlarge)[ right =of middle]{\textcolor{Privacy\\Loss}};
		\node (source) [right=of middle,draw=black,thick,rectangle,fill=red!30]{Privacy\\Loss};
		\draw[outpt](middle)--(enlarge);
		\end{tikzpicture}
	}
\end{frame}


\begin{frame}
	\frametitle{Data}
	\vspace{0.7cm}
	\begin{itemize}
		\item  $\blacktriangleright$ \textbf{Device Analyzer}: global dataset from mobile devices with
		system information, cellular and wireless location
		\begin{itemize}
			\item --- cellular identifiers pseudonymized per handset
			\item --- collected by the University of Cambridge, under ethics committee approval
		\end{itemize}
		\vspace{0.2cm}
		\item $\blacktriangleright$ $ \mathbf{1500} $ \textbf{users} with the most cellular location datapoints
		\begin{itemize}
		\item --- average
		of $ 430 $ days of observation, 
		\item --- more than $ 200 $ regions of interest
		\end{itemize}
		%\vspace{0.2cm}
		%\item cids pseudonymized per handset
		%\item $1$st order mobility networks for networks with $>50$ nodes
	\end{itemize}
\end{frame}



\begin{frame}
	\frametitle{Mobility networks}
	\begin{tcolorbox}[colback=green!5,colframe=white!40!black]
		\textbf{Graphs} with nodes corresponding to
	ROIs and edges to recorded transitions between ROIs
	\end{tcolorbox}
	\begin{itemize}
		\item $\blacktriangleright$  \textbf{Network Order Selection} via Markov chain modeling of
	sequential data {\footnotesize \citep{scholtes2017network}}
		\item $\blacktriangleright$  \textbf{Node attributes} with no temporal/geographic information
		\item $\blacktriangleright$  \textbf{Edge weights} corresponding to frequency of transitions
		\item  $\blacktriangleright$  Location pruning to \textbf{top$-N$ networks} by keeping the most frequently visited regions in user's routine
	\end{itemize}
\end{frame}



\begin{frame}
	\frametitle{Empirical statistics}
	Graphs with:
	\begin{itemize}
		\vspace{0.2cm}
		\item --- heavy-tailed degree distributions
		\vspace{0.2cm}
		\item --- large number of rarely repeated transitions
		\vspace{0.2cm}
		\item --- small number of frequent transitions
		%\item high recurrence rate
	\end{itemize}
\end{frame}


\begin{frame}
	\frametitle{Privacy framework}
	\begin{center}
	\textbf{\emph{$k-$anonymity}  via  \emph{graph isomorphism}}
	\end{center}\\
\begin{tcolorbox}[colback=green!5,colframe=white!40!black,title=Graph $k-$anonymity] 
	 is the minimum  cardinality of isomorphism
	classes within a population of graphs
\end{tcolorbox}
	\begin{flushright}
	Adapted from $k$-anonymity definition in~\Fontvi\citep{sweeney2002k}\hfill
    \end{flushright}
\end{frame}



\begin{frame}
	\frametitle{Identifiability of  top$-N$ mobility networks}
		\begin{columns}
		\begin{column}[t]{0.49\textwidth}
			\centering \includegraphics[width=1.05\textwidth]{figs/k_anonymity_dir_.pdf}
			\\ \textbf{directed}
		\end{column}
		\begin{column}[t]{0.49\textwidth}
			\centering \includegraphics[width=1.05\textwidth]{figs/k_anonymity_undirected_.pdf}
			\\ \textbf{undirected}
		\end{column}
	\end{columns}
\begin{itemize}
	\item  $\blacktriangleright$ $\mathbf{15}$ and $\mathbf{19}$ locations suffice to form uniquely identifiable
\textbf{directed} and \textbf{undirected} networks 
\item  $\blacktriangleright$ $\mathbf{5}$ and $\mathbf{8}$  are the corresponding theoretical upper bounds
\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Anonymity size of  top$-N$ mobility networks}
	\vspace{.15cm}
	\begin{center}
			\includegraphics[width=7.5cm, height=5.7cm]{figs/anonymity_distribution_.pdf}
	\end{center}
	\begin{itemize}
		\item  $\blacktriangleright$ small isomorphism clusters for even very few locations
		\item  $\blacktriangleright$ median anonymity becomes one for network
sizes of 5 and 8 in directed and undirected networks
respectively
\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{{\Fontit Recurring patterns in typical user's  mobility}}
	\begin{center}
	\begin{columns}
		\begin{column}[t]{0.49\textwidth}
				\centering \includegraphics[width=4cm, height=3cm]{figs/2_firsthalf_.pdf}
				\\1st half of the observation period
		\end{column}
			\begin{column}[t]{0.49\textwidth}
				\centering \includegraphics[width=4cm, height=3cm]{figs/2_secondhalf_.pdf}
				\\2nd half of the observation period 
	\end{column}
	\end{columns}
	\end{center}
\\
		 \footnotesize{shown edges correspond to the $10\%$ most frequent transitions in the respective observation window}
\end{frame}


\begin{frame}
	\frametitle{Threat Model}
\begin{center}
	\begin{tikzpicture}
	[scale=.5,auto=left,every node/.style={circle,fill=blue!50, scale=0.7,-latex'}]
	\node (n2) at (0,0.4)  {};
	\node (n3) at (-2,-0.5)  {};
	\node (n5) at (2.3,-0.4)  {};
	\node (n1) at (1,-0.5) {};
	\node (n6) at (0.2,-1.6)  {};
	\node (n7) at (3,-1.7)  {};
	\node (n8) at (4,-0.4)  {};
	\node (n10) at (-1,-1.6)  {};
	\foreach \from/\to in {n5/n1,n1/n2,n2/n5,n2/n3,n10/n8, n10/n1, n2/n10, n7/n3, n7/n8, n6/n5, n7/n6}
	\draw (\from) -- (\to);
	\end{tikzpicture}
	\hspace{0.5cm}
	\begin{tikzpicture}
	[scale=.5,auto=left,every node/.style={circle,fill=blue!50, scale=0.7,-latex'}]
	\node (n2) at (0,0.4)  {};
	\node (n3) at (-2,-0.5)  {};
	\node (n4) at (1.5,-1.8) {};
	\node (n5) at (2.3,-0.4)  {};
	\node (n6) at (0.2,-1.6)  {};
	\node (n7) at (3,-1.7)  {};
	\node (n8) at (4,-0.4)  {};
	\node (n9) at (2,0.5)  {};
	\node (n10) at (-1,-1.6)  {};
	\foreach \from/\to in {n6/n4,n4/n5,n2/n5,n2/n3,n3/n4,n8/n4,n9/n4,n10/n8,n9/n5,  n3/n9, n4/n9, n2/n10, n7/n3, n7/n8}
	\draw (\from) -- (\to);
	\end{tikzpicture}
	
	\vspace{1cm}
	
	\begin{tikzpicture}
	[scale=.5,auto=left,every node/.style={circle,fill=orange!60, scale=0.7,-latex'}]
	\node (n4) at (1.2,-1.5) {};
	\node (n5) at (2.3,-0.7)  {};
	\node (n1) at (1.1,-0.2) {};
	\node (n6) at (0,-1.6)  {};
	\node (n7) at (2.7,-1.9)  {};
	\node (n8) at (3.6,-0.8)  {};
	\node (n9) at (2.1,0.7)  {};
	\node (n10) at (-1.1,-1.4)  {};
	\foreach \from/\to in {n6/n4,n4/n5,n8/n4,n10/n8,n9/n5, n10/n1, n4/n9,  n7/n8}
	\draw (\from) -- (\to);
	\end{tikzpicture}
	\hspace{0.5cm}
	\begin{tikzpicture}
	[scale=.5,auto=left,every node/.style={circle,fill=orange!60, scale=0.7,-latex'}]
	\node (n4) at (1.2,-1.5) {};
	\node (n5) at (2.3,-0.7)  {};
	\node (n1) at (1.1,-0.2) {};
	\node (n6) at (0,-1.6)  {};
	\node (n7) at (2.7,-1.9)  {};
	\node (n8) at (3.6,-0.8)  {};
	\node (n9) at (2.1,0.7)  {};
	\node (n10) at (-1.1,-1.4)  {};
	\foreach \from/\to in {n6/n4,n4/n5,n5/n1,n8/n4,n9/n4,n10/n8,n9/n5, n7/n8, n6/n7, n10/n4}
	\draw (\from) -- (\to);
	\end{tikzpicture}
	
	
	\vspace{1cm}
	
	\begin{tikzpicture}
	[scale=.5,auto=left,every node/.style={circle,fill=black!80!green!80!, scale=0.7,-latex'}]
	\node (n2) at (0,0.4)  {};
	\node (n3) at (-2,-0.5)  {};
	\node (n4) at (1.5,-1.8) {};
	\node (n5) at (2.3,-0.4)  {};
	\node (n1) at (1,-0.5) {};
	\node (n7) at (3,-1.7)  {};
	\node (n8) at (4,-0.4)  {};
	\node (n9) at (2,0.5)  {};
	\foreach \from/\to in {n4/n5,n5/n1,n1/n2,n3/n4,n8/n4,n9/n4,n9/n5, n3/n9, n4/n9, n1/n3, n7/n3, n7/n8}
	\draw (\from) -- (\to);
	\end{tikzpicture}
	\hspace{0.5cm}
	\begin{tikzpicture}
	[scale=.5,auto=left,every node/.style={circle,fill=black!80!green!80!, scale=0.7,-latex'}]
	\node (n2) at (0,0.4)  {};
	\node (n3) at (-2,-0.5)  {};
	\node (n4) at (1.5,-1.8) {};
	\node (n5) at (2.3,-0.4)  {};
	\node (n1) at (1,-0.5) {};
	\node (n7) at (3,-1.7)  {};
	\node (n8) at (4,-0.4)  {};
	\node (n9) at (2,0.5)  {};
	\foreach \from/\to in {n4/n5,n5/n1,n1/n2,n2/n3,n3/n4,n8/n4,n9/n4,n9/n5,  n3/n9, n4/n9,  n1/n3, n7/n3, n7/n8}
	\draw (\from) -- (\to);
	\end{tikzpicture}
\end{center}
\end{frame}


\begin{frame}
	\frametitle{Threat Model}
	\begin{center}
		\begin{tikzpicture}
		\node[block] at (0,2) {{\small \textbf{DISCLOSED IDs}} \\ $\mathcal{G}_\text{train}$};
		\end{tikzpicture}
		\begin{tikzpicture}
		\node[block] at (0,2) {{\small \textbf{UNDISCLOSED IDs}}\\ $\mathcal{G}_\text{test}$};
		\end{tikzpicture}
		\begin{tikzpicture}
		[scale=.5,auto=left,every node/.style={circle,fill=blue!50, scale=0.7,-latex'}]
		\node (n2) at (0,0.4)  {};
		\node (n3) at (-2,-0.5)  {};
		\node (n5) at (2.3,-0.4)  {};
		\node (n1) at (1,-0.5) {};
		\node (n6) at (0.2,-1.6)  {};
		\node (n7) at (3,-1.7)  {};
		\node (n8) at (4,-0.4)  {};
		\node (n10) at (-1,-1.6)  {};
		\foreach \from/\to in {n5/n1,n1/n2,n2/n5,n2/n3,n10/n8, n10/n1, n2/n10, n7/n3, n7/n8, n6/n5, n7/n6}
		\draw (\from) -- (\to);
		\end{tikzpicture}
		\hspace{0.7cm}
		\begin{tikzpicture}
		[scale=.5,auto=left,every node/.style={circle,fill=black!80, scale=0.7,-latex'}]
		\node (n2) at (0,0.4)  {};
		\node (n3) at (-2,-0.5)  {};
		\node (n4) at (1.5,-1.8) {};
		\node (n5) at (2.3,-0.4)  {};
		\node (n6) at (0.2,-1.6)  {};
		\node (n7) at (3,-1.7)  {};
		\node (n8) at (4,-0.4)  {};
		\node (n9) at (2,0.5)  {};
		\node (n10) at (-1,-1.6)  {};
		\foreach \from/\to in {n6/n4,n4/n5,n2/n5,n2/n3,n3/n4,n8/n4,n9/n4,n10/n8,n9/n5,  n3/n9, n4/n9, n2/n10, n7/n3, n7/n8}
		\draw (\from) -- (\to);
		\end{tikzpicture}
		
		\vspace{0.1cm}
		
		\begin{tikzpicture}
		[scale=.5,auto=left,every node/.style={circle,fill=orange!60, scale=0.7,-latex'}]
		\node (n4) at (1.2,-1.5) {};
		\node (n5) at (2.3,-0.7)  {};
		\node (n1) at (1.1,-0.2) {};
		\node (n6) at (0,-1.6)  {};
		\node (n7) at (2.7,-1.9)  {};
		\node (n8) at (3.6,-0.8)  {};
		\node (n9) at (2.1,0.7)  {};
		\node (n10) at (-1.1,-1.4)  {};
		\foreach \from/\to in {n6/n4,n4/n5,n8/n4,n10/n8,n9/n5, n10/n1, n4/n9,  n7/n8}
		\draw (\from) -- (\to);
		\end{tikzpicture}
		\hspace{0.5cm}
		\begin{tikzpicture}
		[scale=.5,auto=left,every node/.style={circle,fill=black!80, scale=0.7,-latex'}]
		\node (n4) at (1.2,-1.5) {};
		\node (n5) at (2.3,-0.7)  {};
		\node (n1) at (1.1,-0.2) {};
		\node (n6) at (0,-1.6)  {};
		\node (n7) at (2.7,-1.9)  {};
		\node (n8) at (3.6,-0.8)  {};
		\node (n9) at (2.1,0.7)  {};
		\node (n10) at (-1.1,-1.4)  {};
		\foreach \from/\to in {n6/n4,n4/n5,n5/n1,n8/n4,n9/n4,n10/n8,n9/n5, n7/n8, n6/n7, n10/n4}
		\draw (\from) -- (\to);
		\end{tikzpicture}
		
		
		\vspace{0.1cm}
		
		\begin{tikzpicture}
		[scale=.5,auto=left,every node/.style={circle,fill=black!80!green!80!, scale=0.7,-latex'}]
		\node (n2) at (0,0.4)  {};
		\node (n3) at (-2,-0.5)  {};
		\node (n4) at (1.5,-1.8) {};
		\node (n5) at (2.3,-0.4)  {};
		\node (n1) at (1,-0.5) {};
		\node (n7) at (3,-1.7)  {};
		\node (n8) at (4,-0.4)  {};
		\node (n9) at (2,0.5)  {};
		\foreach \from/\to in {n4/n5,n5/n1,n1/n2,n3/n4,n8/n4,n9/n4,n9/n5, n3/n9, n4/n9, n1/n3, n7/n3, n7/n8}
		\draw (\from) -- (\to);
		\end{tikzpicture}
		\hspace{0.5cm}
		\begin{tikzpicture}
		[scale=.5,auto=left,every node/.style={circle,fill=black!80, scale=0.7,-latex'}]
		\node (n2) at (0,0.4)  {};
		\node (n3) at (-2,-0.5)  {};
		\node (n4) at (1.5,-1.8) {};
		\node (n5) at (2.3,-0.4)  {};
		\node (n1) at (1,-0.5) {};
		\node (n7) at (3,-1.7)  {};
		\node (n8) at (4,-0.4)  {};
		\node (n9) at (2,0.5)  {};
		\foreach \from/\to in {n4/n5,n5/n1,n1/n2,n2/n3,n3/n4,n8/n4,n9/n4,n9/n5,  n3/n9, n4/n9,  n1/n3, n7/n3, n7/n8}
		\draw (\from) -- (\to);
		\end{tikzpicture}
	\end{center}
	
	\begin{tikzpicture}[remember picture,overlay]
	\draw[line width=1pt] (5.4,0.5) -- (5.4,6.5);
	\end{tikzpicture}
Assumptions
\begin{itemize}
	\item  $\blacktriangleright$ closed-world
	\item  $\blacktriangleright$ partition point for each user randomly $\in (0.3, 0.7)$ of total obs. period
	\item  $\blacktriangleright$ state frequency information
\end{itemize}	
\end{frame}



\begin{frame}
	\frametitle{Threat Model}
	\begin{columns}
		\begin{column}{0.6\textwidth}
				\includegraphics[scale=0.5]{figs/network_setting.png}
		\end{column}
			\begin{column}{0.4\textwidth}
				\hspace{1cm}
		\includegraphics[width=3cm, height=3cm]{figs/adv1.png}
	\end{column}	
	\end{columns}
	%\centering
	%\includegraphics[width=5cm, height=7cm]{figs/tm1.png}
\end{frame}


\begin{frame}
	\frametitle{Attacks: Uninformed Adversary}
	\begin{columns}
		\begin{column}{0.2\textwidth}
			\begin{figure}
			\includegraphics[scale=0.25]{figs/unlabeled_networks.png}
			\end{figure}
		\end{column}
		\begin{column}{0.7\textwidth}
			\begin{center}
				\includegraphics[width=2.3cm, height=2.7cm]{figs/adv_uninformed.png}
				\begin{tikzpicture}
				\node[cloud, cloud puffs=15.7, minimum width=.3cm, minimum height=.3cm, inner sep=0pt,
				text width=4.2cm,draw] (cloud) at (-0.2,-0.2) {
					\begin{tabular}{}
					{\mbox{assign identities at random}} \\
				 {\mbox{\textbf{expected rank}}=$\mathbf{|\mathcal{L}|/2}$}
					\end{tabular}
				};
				\end{tikzpicture}
			\end{center}
		\end{column}	
	\end{columns}
\end{frame}

%$P\big(l_{G'}= l_{G_i}\big) = 1/|\mathcal{L}|, \\ \mbox{for every }  G_i \in \mathcal{G}_{\text{train}}$\\


\begin{frame}
	\frametitle{Attacks: Informed Adversary}
	\begin{columns}
		\begin{column}{0.2\textwidth}
			\includegraphics[scale=0.25]{figs/network_setting.png}
		\end{column}
		\begin{column}{0.8\textwidth}
\thinkA{
	\begin{tabular}{}
		 \mbox{assign \textbf{more probability mass}}\\
		 \mbox{to identities with \textbf{higher structural similarity}}
	\end{tabular}
}
\includegraphics[width=2.3cm, height=2.7cm]{figs/adv_informed.png}
		\end{column}	
	\end{columns}
	%\centering
	%\includegraphics[width=5cm, height=7cm]{figs/tm1.png}
\end{frame}


\begin{frame}
	\frametitle{Attacks: Informed Adversary}
	\begin{itemize}
		\item  $\blacktriangleright$ Posterior probability \\	
		\begin{tabular}{ ll }
		$P\big(l_{G'} =l_{G_i}$&$|\mathcal{G}_{\text{train}}, K\big) \propto  f\big(K(G_i, G')\big)$,\\ 
		& \text{for every}   $G_i \in \mathcal{G}_{\text{train}}$\\
		& \textcolor{red}{$K$ : graph similarity metric},\\ & $f$ : non-decreasing
		\end{tabular} 
		\item  $\blacktriangleright$ Privacy Loss 
		\begin{align*}
		\begin{split}
		PL\big(G';\mathcal{G}_{\text{train}}, K\big) = \frac{ P\big(l_{G'}= l_{G'_\text{true}}|\mathcal{G}_{\text{train}}, K\big)}{P\big(l_{G'}= l_{G'_\text{true}}\big)} -1
		\end{split}
		\end{align*}
	\end{itemize}
\end{frame}



\begin{frame}
	\frametitle{Graph Similarity Functions}
	\begin{block}{Graph Kernels} Express similarity as inner product of vectors with graph statistics \textcolor{black}{\Fontit{\citep{Vishwanathan2010}}}
		\begin{itemize}
			\pause
			\vspace{0.2cm}
			\item   $\blacktriangleright$ on \textbf{Atomic Substructures} (e.g. Shortest-Paths, Weisfeiler-Lehman subtrees) \\
			\pause
			\vspace{0.2cm}
			\item  $\blacktriangleright$ \textbf{Deep Kernels}: \Fontit{\citep{yanardagV15}} \\
			 additionally learn an encoding similarities between substructures
		\end{itemize}
	\end{block}	
\end{frame}


\begin{frame}
	\centering
	\frametitle{Kernel-assisted Ranking}
	\begin{center}
		\begin{figure}
			\begin{subfigure}{\textwidth}
				\centering
				\includegraphics[width=.7\linewidth]{figs/kernels_evaluation}
			\end{subfigure}%
			\begin{subfigure}{\textwidth}
				\centering
				\includegraphics[width=.7\linewidth]{figs/rank_correct_}
			\end{subfigure}
			\label{fig:test}
		\end{figure}
	\end{center}
\vspace{0.01mm}
	\begin{itemize}
		\item mean correct rank under  \textbf{DSP} (random) at $\mathbf{140}$ ($750$)
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Privacy Loss}
	\begin{columns}
		\begin{column}{0.55\textwidth}
			\includegraphics[width=7cm, height=6cm]{figs/posterior_privloss_.pdf}
		\end{column}
		\begin{column}{0.45\textwidth}
			\vspace{1cm}
			\begin{itemize}
				\item $f(\cdot)=\frac{1}{rank(\cdot)}$
				\item median $ = 2.52 $
			\end{itemize}
		 \visible<.(1)>{\includegraphics[width=3cm, height=3cm]{figs/adv_celebrating}}
		\end{column}
	\end{columns}
\end{frame}


\begin{frame}
	\frametitle{Takeaways}
	
	\begin{itemize}
		\item  $\blacktriangleright$ \textbf{Location pruning} does not necessarily make network more privacy-preserving
		\vspace{0.1cm}
		\item  $\blacktriangleright$ Including \textbf{rare transitions} in longitudinal mobility did \textbf{not} add discriminative
information
		\vspace{0.1cm}
		\item  $\blacktriangleright$ Deanonymization is assisted by \textbf{frequency of
locations}, \textbf{directionality of transitions}
	\end{itemize}
\end{frame}



\begin{frame}
	\frametitle{Summary of findings}
	\begin{block}{}%{We investigated privacy properties of \textbf{graph
	%			representations} of longitudinal mobility}
		%\begin{itemize}
		%	\item New deanonymization attack on mobility data using
		%	\textbf{structural similarity} with historical information
		%	\item Evaluation on \textbf{large dataset of cell-tower location traces}
				\begin{itemize}
				\item $\blacktriangleright$ graph representations of mobility display \textbf{distinct structure},
				\textbf{even for small number of nodes}
				\item  $\blacktriangleright$ \textbf{$ \mathbf{< 20}$ locations} are enough to identify uniquely a
				population of \mbox{\textbf{$ \mathbf{1500} $ users}}
				\item  $\blacktriangleright$ \textbf{kernel-based distance functions} can quantify similarity in absence
				of location semantics and fine-grained temporal information
				\item  $\blacktriangleright$ probabilistic deanonymization using similarity with historical
				data can achieve \textbf{median success probability $ \mathbf{3.5\times} $ higher
					than a random mechanism}
			\end{itemize}
			
		%\end{itemize}
	\end{block}
\end{frame}



\begin{frame}
	\frametitle{Future Directions}
	\begin{itemize}
		\item  $\blacktriangleright$ \textbf{Geometry of kernel feature spaces}: high dimensional space with meaningful neighborhood relations 
		\item  $\blacktriangleright$ \textbf{Other graph similarity techniques}: network alignment,
		persistent cascades, frequent/discriminative substructure mining, anonymous walks, spectral representations
		\item  $\blacktriangleright$ Application to \textbf{other categories of sequential datasets}: web browsing
		behaviour, smartphone app usage
		\item  $\blacktriangleright$ \textbf{Formal privacy guarantees} for mobility networks
		\item  $\blacktriangleright$ \textbf{Utility preserving defense mechanisms}: kernel-agnostic defense, randomisation of node
		\item  $\blacktriangleright$ \textbf{Generative mechanisms} for synthetic traces with anonymity guarantees
		attributes, perturbations of edges, node removal
	\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% NEURIPS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
	\LARGE{\textbf{II. Bayesian Pseudocoresets}}
\end{frame}

\begin{frame}
	\frametitle{Large Scale Inference}
	\begin{itemize}
		\item $\blacktriangleright$ Update beliefs about unknowns given massive data \\
		\hfill \includegraphics[width=2cm, height=2cm]{figs/Bayes.png} 
		\begin{align*}
		\pi(\theta|x) = \frac{1}{Z} \pi(x|\theta) \pi(\theta) 
		\end{align*}
		\pause
		\hfill
			\textcolor{darkred}{Hard to compute!}
		\pause
		\item $\blacktriangleright$ \textbf{Intuition:} In large data there is a lot of redundancy. Hence, compress the dataset into an informative summary and perform inference thereon
	\end{itemize}
	\begin{figure}{\textwidth}
		\centering
		\includegraphics[width=.7\linewidth]{figs/coresets_high_level.png}
		\label{fig:coresets_high_level}
	\end{figure}
\end{frame}

\begin{frame}
	\frametitle{Bayesian Coresets}
	Riemannian Coresets~\citep{campbell19neurips}
	\begin{figure}
		\includegraphics[width=1.\linewidth]{figs/sparsevi_posterior.png}
	\end{figure}
	Sparse Variational Inference
	\begin{align*}
	 w^\star = \argmin_{w\in\reals^N} \kl{\pi_w}{\pi}\label{eq:coresets-vi} \quad \quad
	\text{s.t.} \quad w \geq 0, \; \|w\|_0 \leq M
	\end{align*}
\end{frame}

\begin{frame}
	\frametitle{Limitation I: High-dimensional data}
	\begin{tcolorbox}[colback=green!5,colframe=white!40!black,title=KL Lowner Bound in Gaussian mean inference via coresets]  
		Suppose we use $(X_n)_{n=1}^N \distiid \distNorm(0, I)$ in $\reals^d$ to perform posterior inference in a Bayesian model
		with prior 
		$\mu \dist \distNorm(0, I)$ and likelihood
		$(X_n)_{n=1}^N  \distiid \distNorm(\mu, I).$
		Then $\forall M < d$ and $\delta \in[0, 1]$, 
		with probability at least $1-\delta$ the optimal size-$M$ coreset $w^\star$ satisfies
		\[
		\kl{\pi_{w^\star}}{\pi} \geq \frac{1}{2}\frac{N-M}{1+N}F_{d-M}^{-1}\left(\delta{N\choose M}^{-1}\right),
		\]
		where $F_{k}$ is the CDF of a $\chi^2$ random variable with $k$ degrees of freedom.
	\end{tcolorbox}
\end{frame}


\begin{frame}
	\frametitle{Limitation I: High-dimensional data}
	\begin{figure*}
		\centering
		\includegraphics[width=.8\linewidth]{figs/klbound.png}
	\end{figure*}
\end{frame}

\begin{frame}
	\frametitle{Limitation I: High-dimensional data}
	In Gaussian mean inference coreset covariance and mean are 
	\begin{align*}
	\Sigma_w &=\left(1+\sum_{n=1}^N w_n\right)^{-1} I, & \mu_w &= \Sigma_w \sum_{n=1}^N w_n X_n.\label{eq:gausswpost}
	\end{align*}
	
	which can be replicated via a point 
	\begin{align*}
	& U = \left(\sum_{n=1}^Nw_n\right)^{-1}\sum_{n=1}^N w_nX_n
	& \text{with weight}\;\; \sum_{n=1}^N w_n.
	\end{align*}
	True posterior can be exactly computed via a \emph{single pseudopoint}
	\begin{align*}
	& U = \frac{1}{N}\sum_{n=1}^N X_n & \text{with weight}\;\; N.
	\end{align*}
\end{frame}

\begin{frame}
	\frametitle{Limitation II: Privacy}
	Existing constructions are incompatible with formal privacy definitions, as subset
	of datapoints gets revealed \\
	\pause 
	For sensitive datasets, summarization
	should use synthetic data
	\begin{figure*}
		\includegraphics[width=1\linewidth]{figs/dp_summarize.png}
	\end{figure*}
\end{frame}

\begin{frame}
	\frametitle{Bayesian Pseudocoresets}
	Pseudocoreset posterior
	\begin{align*}
		\pi_{u,w}(\theta) = \frac{1}{Z(u, w)} \exp \left( \sum_{m=1}^{M} w_m f(u_m,\theta) \right) \pi_0(\theta)
	\end{align*}
	where $(u_m)_{m=1}^{M}$ are \textbf{learnable pseudodata points}.\\
	\pause
	Variational inference over \textbf{pseudodata locations} and \texbf{weights}
	\begin{align*}
	u^\star, w^\star = \argmin_{\substack{u\in\reals^{d \times M}}, w\in\reals^M_+}\,\, \kl{\pi_{u,w}}{\pi}. 
	\end{align*}
\end{frame}


\begin{frame}
	\frametitle{Pseudocoresets Construction}
	\begin{align*}
	u^\star, w^\star = \argmin_{\substack{u\in\reals^{d \times M}}, w\in\reals^M_+}\,\, \kl{\pi_{u,w}}{\pi}. 
	\end{align*}	
	
	Gradients \wrt variational parameters are tractable 
	\begin{align*}
	\nabla_{u_m}\mathrm{D}_{\mathrm{KL}} = -w_m\cov_{u,w}\left[(\nabla_u f)(u_m), f(x, \theta)^T1 - f(u, \theta)^Tw\right]
	\end{align*}
	\begin{align*}
	 \nabla_w\mathrm{D}_{\mathrm{KL}} = - \cov_{u,w}[f(u, \theta) ,f(x, \theta)^T1 - f(u, \theta)^Tw]
	\end{align*}
\end{frame}


\begin{frame}
	\frametitle{Black-box Batch Construction of Bayesian Pseudocoreset}
	\vspace{-0.7cm}
  \scriptsize 
	\begin{algorithmic}[1]
		\Procedure{PSVI}{$f(\cdot, \cdot), \pi_0, x, M, B, S, T, (\gamma_t)_{t=1}^{\infty}$}
		\LineCommentIndent{Initialize using a uniformly chosen subset of the full dataset}
		\State $N \gets $ \# datapoints in $x, \quad \mcB\dist\distUnifSubset\left([N], M\right), \quad \mcB \defined \left\{b_1, \dots, b_M\right\}$ 
		\State $u_m \gets x_{b_m}, \quad w_m \gets \nicefrac{N}{M}, \quad m=1, \dots, M$
		\For{$ t = 1, \ldots, T$}
		\LineCommentIndent{\mbox{Take $S$ samples from current pseudocoreset posterior}}
		\State $(\theta)_{s=1}^{S}  \distiid \pi_{u,w}$ where $\pi_{u,w}(\theta) \propto \exp\left(\sum_{m=1}^Mw_m f(u_m, \theta)\right)\pi_0(\theta)$
		\State $\mcB\dist\distUnifSubset\left([N], B\right)$
		\Commenttriangle{$\triangleright$ \mbox{Get a minibatch of $B$ points from the full dataset}}
		\For{$s = 1, \dots, S$} 	\Commenttriangle{$\triangleright$ \mbox{Compute (gradient) log-likelihood discretizations}}
		\State $g_{s} \gets \left( f(x_b, \theta_s ) - \nicefrac{1}{S}\sum_{s'=1}^S f(x_b, \theta_{s'}) \right)_{b\in\mcB} \in \reals^B$ 
		\State $\tg_s\gets \left(f(u_m, \theta_s) - \nicefrac{1}{S}\sum_{s'=1}^S f(u_m, \theta_{s'})\right)_{m=1}^M\in\reals^M$
		\For{$m = 1, \dots, M$}
		\State $\tildeh_{m,s} \gets \nabla_u f(u_m, \theta_s) - \nicefrac{1}{S}\sum_{s'=1}^S \nabla_u f(u_m, \theta_{s'})\in\reals^d$
		\EndFor
		\EndFor
		\State $\hat\nabla_w \gets -\nicefrac{1}{S}\sum_{s=1}^S \tg_s\left(\nicefrac{N}{B} g_s^T1 - \tg_s^Tw\right)$\Commenttriangle{$\triangleright$ \mbox{Compute MC gradients for $w$ and $u$}}
		\For{$m = 1,\dots, M$}
		\State $\hat\nabla_{u_m} \gets -w_m\nicefrac{1}{S}\sum_{s=1}^S \tildeh_{m,s}\left(\nicefrac{N}{B} g_s^T1 - \tg_s^Tw\right)$
		\EndFor
		\State $w \gets \max(w - \gamma_t\hat\nabla_w, 0)$ \Commenttriangle{$\triangleright$ \mbox{Take stochastic gradient step in $w$ and $u$}}
		\For{$m = 1,\dots, M$}
		\State $u_m \gets u_m - \gamma_t\hat\nabla_{u_m}$
		\EndFor
		\EndFor
		\State\Return $w$, $(u_m)_{m=1}^M$
		\EndProcedure		 
	\end{algorithmic}
\normalsize
	\end{frame}


\begin{frame}
	\frametitle{Differentially Private Construction of Bayesian Pseudocoreset}
	\vspace{-0.7cm}
	\begin{columns}
		\begin{column}[t]{0.4\textwidth}
		\begin{itemize}
			\item $\blacktriangleright$ \textbf{Initialize} pseudodata to points sampled from the statistical model
			\item $\blacktriangleright$ \textbf{Iterate} Compute private estimates of $\nabla_w\mathrm{D}_{\mathrm{KL}}$, $\nabla_u\mathrm{D}_{\mathrm{KL}}$ via applications of the \textbf{subsampled Gaussian mechanism}~\citep{abadi16} and take gradient steps
		\end{itemize}
		\end{column}
	\pause
		\begin{column}[t]{0.6\textwidth}
		\begin{itemize}
			\item --- Moments accountant for tight $(\veps, \delta)$-DP guarantees estimation
			\item --- No extra privacy cost when sampling from the private pseudocoreset posterior
			\item --- No requirement for global sensitivity computation
			\item --- Adaptive gradient clipping guided by statistics of the pseudopoints potentials
			\item --- Constructed private pseudocoresets can be further queried ad infinitum for data analysis without privacy guarantees weakening
		\end{itemize}
		\end{column}
	\end{columns}
\end{frame}

\begin{frame}
	\frametitle{Experiments I~(High-dimensional Gaussian mean inference)}
	\centering 
	500 dimensions, 1K datapoints\\
	\includegraphics[width=.49\textwidth]{figs/d500_pts_combined.png}
	\includegraphics[width=.49\textwidth]{figs/d500_KLDvsCstSize.png}
\end{frame}

\begin{frame}
	\frametitle{Experiments II~(Bayesian Linear Regression)}
	\centering
	\begin{columns}
		\begin{column}{.5\textwidth}
		Synthetic dataset
		\\101 dimensions,
		\\2K datapoints
		\end{column}
		\begin{column}{.5\textwidth}
		\includegraphics[width=1.\textwidth]{figs/linregKLDvsCstSize.png}
		\end{column}
	\end{columns}
\end{frame}

\begin{frame}
	\frametitle{Experiments III~(Bayesian Logistic Regression)}
	\centering 
	2-237 dimensions, 500-100K datapoints\\
	\includegraphics[width=.3\textwidth]{figs/wsanta100K_KLDvssz.png}
	\includegraphics[width=.3\textwidth]{figs/wds1100_KLDvssz.png}
	\includegraphics[width=.3\textwidth]{figs/wfma_KLDvssz.png} \\
	\includegraphics[width=.3\textwidth]{figs/wsanta100K_privacy.png}
	\includegraphics[width=.3\textwidth]{figs/wds1100_privacy.png}
	\includegraphics[width=.3\textwidth]{figs/wfma_privacy.png} 
	\small $\delta=\frac{1}{N}$
	\normalsize
\end{frame}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% WSDM
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\begin{frame}
	\LARGE{\textbf{III. $\beta$-Cores: Robust Large-Scale Bayesian Data Summarization in the Presence of Outliers}}
\end{frame}




\begin{frame}
	\frametitle{Introduction}
	\begin{itemize}
		\item $\blacktriangleright$ Large-scale ML systems collect growing massive datasets contributed from multiple sources
		\item $\blacktriangleright$ \textbf{Requirement 1}: \emph{Scalable} inference via redudancy minimization
		\item $\blacktriangleright$ \textbf{Requirement 2}: \emph{Robustness to statistical misspecification} due to non-conforming observations, or model assumptions   
		\item $\blacktriangleright$ \textbf{$\beta$-Cores} offer \textbf{an integrated approach to data cleaning and scalable black-box inference} with a high degree of automation: Bayesian Summarization robust to contamination potentially existing in the data 
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Background}
	\begin{itemize}
		\item $\blacktriangleright$ \textbf{Outliers detection} is typically treated separately from inference, via computing distances within datapoints~\citep{diakonikolas19, dickens20}, or introducing data redudancies~\citep{raykar10, karger11}. Such approaches scale adversely with data dimensionality and size.
		\item $\blacktriangleright$ \textbf{Robustified Bayesian inference} via 
			\begin{itemize}
				\item --- heavy-tailed data likelihood functions~\citep{huber09, insua12}
				\item --- localization~\citep{definetti61, wang18}
				\item --- robust gradient estimates in MC methods~\citep{bhatia19}
				\item --- datapoint specific weighting~\citep{wang17} 
				\item --- robust statistical divergences~\citep{futami18, knoblauch18, miller19}
			\end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Huber $\eps$-contamination model}
		\begin{columns}
		\begin{column}{.39\textwidth}
			 \hfill \includegraphics[width=1.\textwidth]{figs/Huber.jpg}\\ 
		\end{column}
		\begin{column}{.69\textwidth}
			\hfill \includegraphics[width=.5\textwidth]{figs/contaminated-gaussian.png}\\ 
		\end{column}
	\end{columns}
\definecolor{darkred}{rgb}{0.55, 0.0, 0.0}
$(x_n)_{n=1}^{N}\sim (1-\eps)\cdot G + \eps \color{darkred}{Q}$~\citep{huber92},
where $\eps \in (0,1)$, $G$ follows our statistical assumptions and $\color{darkred}{Q}$ is an arbitrary distribution of outliers.
\end{frame}


\begin{frame}
	\frametitle{Bayes rule: An optimization perspective}
	\centering 
	\begin{columns}
		\begin{column}{.79\textwidth}
	Bayes rule recasted to an optimization problem~\citep{zellner88}
		\end{column}
	\begin{column}{.29\textwidth}
			\hfill \includegraphics[width=1.\textwidth]{figs/Zellner.jpg}\\ 
	\end{column}
	\end{columns}
	\begin{align*}
	\arg \underset{q(\theta) \in \mcP}{\min}  \left(\kl{q(\theta)}{\pi_0(\theta)} + N\EE_{q(\theta)}\left[\kl{\hpi(x)}{\pi(x|\theta)}\right]\right),
	\end{align*}
	giving
	\begin{align*}
	\pi(\theta|x) = \frac{1}{Z'} \exp\left(-\xent{\hpi(x)}{\pi(x|\theta)}\right)\pi_0(\theta),
	\end{align*}
	where $\hpi(x):=\frac{1}{N}\sum_{n=1}^{N}\delta(x-x_n)$ and $\xent{\hpi(x)}{\pi(x|\theta)}:=-\sum_{n=1}^{N}\log\pi(x_n|\theta)$.
\end{frame}


\begin{frame}
		\frametitle{Generalized Bayesian inference via $\beta$-divergence}
		Replace the KL with the \emph{$\beta$-divergence}~\citep{basu98}.\\
		\textbf{$\beta$-posterior}:
		\begin{align*}
		\pi_\beta(\theta|x) \propto \exp\left(-\db{\hpi(x)}{\pi(x|\theta)}\right)\pi_0(\theta),
		\end{align*}
		where \\
		$\db{\hpi(x)}{\pi(x|\theta)} := 
		-\sum_{n=1}^{N}  \underbrace{\left(\frac{\beta+1}{\beta}\pi(x_n|\theta)^{\beta} + \int_{\mcX} \pi(\chi|\theta)^{1+\beta}d\chi\right)}_{:=f_n(\theta)}$, with $\beta>0$.
		\pause
		\begin{itemize}
			\item $\blacktriangleright$ different strengths of influence to each datapoint downweigthing outliers
			\item $\blacktriangleright$ when $\beta \rightarrow 0$ classical Bayesian posterior gets recovered
			\item $\blacktriangleright$ \textbf{same scalability issues with classical Bayesian posterior}
		\end{itemize}
\end{frame}


\begin{frame}
	\frametitle{Coresets for $\beta$-posterior}
	\textbf{Sparse $\beta$-posterior}
	\begin{align*}
	\pi_{\beta,w}(\theta|x) 
	:= \frac{1}{Z(\beta, w)}  \exp\left(\sum_{n=1}^{N}w_nf_n(\theta)\right)\pi_0(\theta)
	\end{align*}
	\pause 
	Variational formulation
	\begin{align*}
	w^{*} = \arg \min_{w\in\reals^{N}} \kl{\pi_{\beta,w}}{\pi_{\beta}} 
	\quad
	\text{s.t.}
	\quad
	w \geq 0,\; ||w||_0 \leq M.
	\end{align*}
	\pause 
	Analytical gradients 
	\begin{align*}
	\nabla_{w}\kl{\pi_{\beta,w}}{\pi_\beta} 
	& = -\cov_{\beta,w}\left[f,(1 -w)^Tf\right],
	\end{align*}
	where $f:=\left[f_1(\theta) \ldots f_N(\theta)\right]^T$.
\end{frame}


\begin{frame}
	\frametitle{Black-box incremental construction of Sparse $\beta$-posterior}
	\vspace{-0.5cm}
	\scriptsize 
	\begin{algorithmic}[1]
		\Procedure{\bcore}{$f,  \pi_0, x, M, B, S, T, (\gamma_t)_{t=1}^{\infty}$, $\beta$}
		\State $w \assign \mathbf{0} \in \reals^{M}$,\;\; $g \assign \mathbf{0} \in \reals^{S \times M}$, \;\;$g' \assign \mathbf{0} \in \reals^{S \times B}$, \;\;$\mcI \assign \emptyset$
		\For{$m =1, \ldots, M $}
		\State $(\theta)_{s=1}^{S}  \distiid \pi_{\beta,w} \propto \exp \left(w^Tf\right) \pi_0(\theta)$ \Commenttriangle{$\triangleright$ Sample from coreset posterior}
		\State $\mcB\dist\distUnifSubset\left([N], B\right)$
		\Commenttriangle{$\triangleright$ \mbox{Get minibatch of $B$ datapoints from the full data}}
		\State $g_{s} \gets \left( f(x_m, \theta_s, \beta ) - \frac{1}{S}\sum_{r=1}^S f(x_m, \theta_{r}, \beta) \right)_{m \in \mcI}\in \reals^M$ 
		\State  $g'_{s} \gets \left( f(x_b, \theta_s, \beta) - \frac{1}{S}\sum_{r=1}^S f(x_b, \theta_{r}, \beta) \right)_{b\in\mcB} \in \reals^B$
		\State $\hcorr \assign \diag \left[ \frac{1}{S} \sum_{s=1}^{S} g_{s}
		{g_{s}}^T\right]^{-\frac{1}{2}} \left(\frac{1}{S} \sum_{s=1}^{S} g_{s} \left(\frac{N}{B}1^T g'_{s} - w^T g_{s}\right)\right) \in \reals^{M}$
		\label{lst:line:core-corr}
		\State $\hcorr' \assign \diag \left[ \frac{1}{S} \sum_{s=1}^{S} g'_{s}
		{g'_{s}}^T\right]^{-\frac{1}{2}} \left(\frac{1}{S} \sum_{s=1}^{S} g'_{s} \left(\frac{N}{B}1^T g'_{s} - w^T g_{s}\right)\right)  \in \reals^{B}$
		\label{lst:line:batch-corr}
		\LineCommentIndent{ Select next via max-correlation}			 
		\State $n^{\star} \assign \arg \underset{n \in [m] \cup [B]}{\max} \left( \left|\hcorr \right| \cdot \vecone[n \in \mcI] + \hcorr' \cdot \vecone [n \notin \mcI]\right)$, \;\;$ \mcI \assign \mcI \cup \{n^{\star}\}$
		\For{$ t = 1, \ldots, T$}
		\State $(\theta)_{s=1}^{S}  \distiid \pi_{\beta,w}(\theta) \propto \exp\left(w^T f\right)\pi_0(\theta)$, $\mcB\dist\distUnifSubset\left([N], B\right)$
		\For{$s = 1, \dots, S$} 	
		\State $g_{s} \gets \left( f(x_m, \theta_s, \beta) - \frac{1}{S}\sum_{r=1}^S f(x_m, \theta_{r}, \beta) \right)_{m\in\mcI} \in \reals^M$  	
		\State $g'_{s} \gets \left( f(x_b, \theta_s, \beta) - \frac{1}{S}\sum_{r=1}^S f(x_b, \theta_{r}, \beta) \right)_{b\in\mcB} \in \reals^B$  	
		\EndFor
		\State $\hat\nabla_w \gets -\frac{1}{S}\sum_{s=1}^S g_s\left( \frac{N}{B} 1^Tg'_s- w^Tg_s\right)$
		\Commenttriangle{$\triangleright$ \mbox{MC gradient for var. params}}
		\State $w \gets \max(w - \gamma_t\hat\nabla_w, 0)$
		%, $\beta \gets \max(\beta - \gamma_t\hat\nabla_\beta, 0)$
		\Commenttriangle{$\triangleright$ \mbox{Take a gradient step}}
		\EndFor
		\EndFor
		\State\Return $w$%, $\beta$
		\EndProcedure		 
	\end{algorithmic}
\normalsize
\end{frame}


\begin{frame}
	\frametitle{Black-box incremental construction of Sparse $\beta$-posterior}
	\begin{itemize}
		\item $\blacktriangleright$ Computational complexity $O(M(M+B)ST)$
		\item $\blacktriangleright$ Extends seamlessly to summarizing observations groups and minibatches 
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Simulated Gaussian Mean Inference under Stuctured Data Contamination}
	\centering
	\includegraphics[width=.67\textwidth, height=4.5cm]{figs/gauss_scatterplot.png}\\
	\includegraphics[width=.3\textwidth]{figs/f0KLDvsCstSize.png}
	\includegraphics[width=.3\textwidth]{figs/f15KLDvsCstSize.png}
	\includegraphics[width=.3\textwidth]{figs/f30KLDvsCstSize.png}
\end{frame}


\begin{frame}
	\frametitle{Bayesian Logistic Regression under Mislabeling and Feature Noise}
	\centering
	\includegraphics[width=.3\textwidth]{figs/adult07_10_25_False_False_ACCvssz.png}
	\includegraphics[width=.3\textwidth]{figs/phish09_10_15_False_False_ACCvssz.png}
	\includegraphics[width=.3\textwidth]{figs/webspam09_10_15_True_False_ACCvssz.png}
	\includegraphics[width=.3\textwidth]{figs/adult07_10_0_False_False_ACCvssz.png}
	\includegraphics[width=.3\textwidth]{figs/phish07_10_0_False_False_ACCvssz.png}
	\includegraphics[width=.3\textwidth]{figs/webspam09_10_0_True_False_ACCvssz.png}
\end{frame}

\begin{frame}
	\frametitle{Neural Linear Regression on Noisy Data Batches}
	\centering
	\includegraphics[width=.49\textwidth]{figs/boston02_01_30_RMSEvssz.png}
	\includegraphics[width=.49\textwidth]{figs/year02_01_30_RMSEvssz.png}
	\includegraphics[width=.49\textwidth]{figs/boston02_01_0_RMSEvssz.png}
	\includegraphics[width=.49\textwidth]{figs/year02_01_0_RMSEvssz.png}
\end{frame}

\begin{frame}
	\frametitle{Efficient Data Acquisition from Subpopulations for Budgeted Inference}
	\centering
	\includegraphics[width=.49\textwidth]{figs/group_diabetes06_10_01_False_ACCvsit.png}
\includegraphics[width=.49\textwidth]{figs/group_diabetes06_10_01_False_ACCvssz.png}
\includegraphics[width=.49\textwidth]{figs/group_diabetes06_10_0_False_ACCvsit.png}
\includegraphics[width=.49\textwidth]{figs/group_diabetes06_10_0_False_ACCvssz.png}	
\end{frame}

\begin{frame}
	\frametitle{Efficient Data Acquisition from Subpopulations for Budgeted Inference}
	\centering
	\includegraphics[width=.49\textwidth]{figs/selected_groups.png}	
	\pause 
	\begin{itemize}
		\item $\blacktriangleright$ Demographic insights conforming with prior results in the literature~\citep{ghorbani19}: older age female group population is most informative for the regression task
		\item $\blacktriangleright$ More effective selection of data subpopulations compared to standard data Shapley valuation~\citep{shapley53}
	\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Future outlook
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\begin{frame}
	\LARGE{\textbf{Future Outlook}}
\end{frame}

\begin{frame}
	\begin{itemize}
		\item $\blacktriangleright$  \texbf{Summarization under human-centric constraints}: Enforce pseudodata interpretability, fairness and right-to-be-forgotten
		\item $\blacktriangleright$  \textbf{Coresets for structured data}: Extend methods to relational datasets and point process data
		\item $\blacktriangleright$ \textbf{Implicit privacy amplification effects of data-informed sampling}: Recent studies have highlighted DP amplification results for uniform samplings. Is a similar analysis possible for importance samplings?
		\item $\blacktriangleright$ \textbf{Summarization for meta-learning}: Extend methods for summaries according to a single statistical model to summaries applicable to sets of models/tasks.
	\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Thanks, acknoledgements
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\begin{frame}
	\Large{\textbf{Thanks} \\ email \href{mailto:dm754@cam.ac.uk}{dm754@cam.ac.uk}  }
	\item \textbf{Colleagues}: Cecilia Mascolo, Trevor Campbell, Alastair R. Beresford
	\item \textbf{Sponsors}: Nokia Bell Labs, Lundgren Fund, Darwin College Cambridge, Department of Computer Science \& Technology
\end{frame}

\begin{frame}[allowframebreaks]{References}
	\tiny %\scriptsize
	\bibliography{references.bib}
\end{frame}

\end{document}

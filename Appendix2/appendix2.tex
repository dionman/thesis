\chapter{Supplement for \bcores{}}
\label{app:app2}

\renewcommand*{\MyPath}{../Appendix2}%


\section{Models}
\label{sec:models}
In this section we present the derivations of \blik{} terms~\cref{eq:b-loss,eq:sl-lik-terms} required over the \bcores{} constructions for the statistical models of our experiments.

\subsection{Gaussian likelihoods}
\label{sec:gauss-lik}

For the \blik{} terms of a multivariate normal distribution, we have 
\[
\pi(x|\mu, \Sigma)^{\beta} = \left((2\pi)^{-\frac{d}{2}}|\Sigma|^{-\frac{1}{2}}\right)^{\beta} \exp\left(-\frac{\beta}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\right),
\]
and, by simple calculus (see also~\textcitec{samek13}),
\[
\int_{\mcX}\pi(\chi|\mu, \Sigma)^{1+\beta}d\chi = \left((2\pi)^{-\frac{d}{2}}|\Sigma|^{-\frac{1}{2}}\right)^{\beta}(1+\beta)^{-\frac{d}{2}}.
\]
Hence, omitting the constant term due to the shift-invariance of potentials entering~\cref{alg:bcores}, we get up to proportionality 
\[
f_n(\mu) 
\propto &  
\frac{1}{\beta} \exp\left(-\frac{\beta}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\right) 
\label{eq:gaussian-beta-lik}.
\]

\subsection{Logistic regression likelihoods}
\label{sec:logreg-lik}
Log-likelihood terms of individual datapoints are given as follows
\[
\log \pi(y_n|x_n, \theta) = -\log\left(1+e^{-y_n z_n^T \theta}\right).
\]
Substituting to~\cref{eq:sl-lik-terms}, for the  \blik{} terms we get
\[
f_n(\theta)& \propto -\frac{1}{\beta}\left(1+e^{-y_n z_n^T \theta}\right)^{-\beta} 
+ \frac{1}{\beta+1} \left( \left(1+e^{- z_n^T \theta}\right)^{-(\beta+1)} + \left(1+e^{z_n^T \theta}\right)^{-(\beta+1)} \right).
\label{eq:logreg-blik}
\]

\subsection{Neural linear regression likelihoods and predictive posterior}
\label{sec:neurlinr-lik}
Recall that in the neural linear regression model, $ \left(y_n - \theta^T z(x_n)\right) \sim \distNorm(0, \sigma^2), \; {n=1,\ldots,N}$.
Then the Gaussian log-likelihoods corresponding to individual observations (after dropping normalization constants),  are written as 
\[
f_n(\theta) = - \frac{1}{2\sigma^2}\left(y_n - \theta^T z(x_n)\right)^2.
\label{eq:neurlinr-logliks}
\]
Assuming a prior $\theta \dist \distNorm(\mu_0, \sigma_0^2 I)$, the coreset posterior is a Gaussian $\pi_w(\theta) = \mcN(\mu_w, \Sigma_w)$, with mean and covariance computable in closed form as follows
\[
&\Sigma_w := \left(\sigma_0^{-2}I + \sigma^{-2} \sum_{m=1}^{M}w_m z(x_m) z(x_m)^T \right)^{-1},\\
&\mu_w := \Sigma_w \left( \sigma_0^{-2} I \mu_0 + \sigma^{-2} \sum_{m=1}^{M} w_m y_m z(x_m) \right).
\label{eq:neurlinr-corest-posterior-params}
\]
By substitution to~\cref{eq:sl-lik-terms} and omitting constants,
the \blik{} terms for our adaptive basis linear regression are written as 
\[
f_n(\theta) \propto e^{-\beta\left(y_n-\theta^Tz(x_n)\right)^2/(2\sigma^2)}.
\label{eq:linreg-blik}
\]
%To simplify notation 
Let $\mcC$ be the output of the coreset applied on a dataset $\mcD$. Hence, in regression problems, the predictive posterior on a test data pair $(x_t, y_t)$ via a coreset is approximated as follows
\[
\pi(y_t|x_t, \mcD) & \approx \pi(y_t|x_t, \mcC)  \notag  \\
&= \int \pi(y_t|x_t,  \theta) \pi(\theta|\mcC) d\theta.  
\label{eq:coreset-postpred}
\]
In the neural linear experiment, 
the predictive posterior is a Gaussian given by the following formula
\[
\pi(y_t|x_t, \mcC) 
& = \distNorm \left(y_t; \mu_w^T z(x_t), \sigma^2 + z(x_t)^T \Sigma_w z(x_t)\right).
\label{eq:neurlinr-pred-posterior}
\]

\section{Characterization of Riemannian coresets' combinatorial optimization objective}
\label{sec:submodularity-characterization}

When optimizing a set function, the property of submodularity is often deemed appealing as it can allow using fast greedy selection policies with provable suboptimality guarantees~\citep{nemhauser78,bach13}. The optimization problem corresponding to our next datapoint selection step of~\cref{eq:opt1} can be equivalently rewritten as follows

\[
 m^{\star} = \arg \max_{m\in [N]} - \kl{\pi_{\beta,w \assign w \cup \{x_{m}\}}}{\pi_{\beta}}.  \label{eq:opt1app} 
\]
Hence---ignoring the coreset datapoints' reweighting step which is treated separately---it is of interest to characterize the properties of the objective function $d:2^{|\mcX|} \rightarrow \reals_{\leq 0}$
\[
d(S):=-\kl{\pi_{\beta,\frac{N}{M}\mcI_{S}}}{\pi_\beta},
\label{eq:setfunction_d}
\]
where $S$ is set of $M$ datapoints appearing with non-zero weight in the coreset. 

Below we give a tight condition for submodularity via second-order differences, which captures its characteristic property of \emph{diminishing returns} for increasing set size.

\ndefn[Submodularity]{ \label{prop:subm_with_2ndorder_diff}
The set function $d$ is submodular if and only if for all $S\subseteq\mcX$ and $x_j, x_k \in \mcX \setminus S$, we have $d(S\cup\{x_j\}) - d(S) \geq d(S\cup\{x_j, x_k\}) - d(S\cup\{x_k\})$.
}

In the next proposition, we demonstrate a problem instance where the necessary and sufficient condition of~\cref{prop:subm_with_2ndorder_diff} is violated for $d$ considered in~\cref{eq:setfunction_d}, hence proving that our objective is \emph{non-submodular} under no further assumptions.
 
\bnprop \label{prop:not-submodular}
The set function $d$ of~\cref{eq:setfunction_d} is non-submodular.
\enprop

\begin{proof}
For convenience let's focus on the case of Gaussian mean inference for the classical Bayesian posterior~($\beta \rightarrow 0$), where the objective can be handily written in closed form. Similar arguments will in principle carry over for arbitrary $\beta$s and statistical models. We recall from~\cref{eq:KL_from_coreset_post_to_exact} that
\[ d(S) &=-\frac{1}{2} \left[ -d\log \left( \frac{1+ N}{1+\frac{N}{M}||\mcI_{S}||_1}\right) - d  + d \left( \frac{1+ N}{1+\frac{N}{M}||\mcI_{S}||_1}\right)
+  (1+N)(\mu_{1} - \mu_{w})^T (\mu_{1} - \mu_{w})\right] \notag  \\
			&=-\frac{1}{2}  (1+N)||\mu_{1} - \mu_{w}||_2^2,
\] 
where
\[
\mu_{1}=\frac{1}{1+N} \sum_{n=1}^{N} x_{n}, \quad \mu_{w}=\frac{1}{1+N} \frac{N}{M} \sum_{x_i \in S} x_{i}.
\]
Let's consider a set of observations containing two mirrored datapoints $x_0, -x_0$, such that $x_0 \neq \mu_1$. Then clearly
\[
&d(S\cup\{x_0\}) - d(S) - d(S\cup\{x_0, -x_0\}) + d(S\cup\{-x_0\}) \notag\\
= & d(S\cup\{x_0\}) + d(S\cup\{-x_0\})- 2 d(\mcX) = d(S\cup\{x_0\}) + d(S\cup\{-x_0\})
<0,
\]
where we have used the fact that $d(S)=d(\mcX)=0$.
\end{proof}


\section{Datasets details}
\label{sec:data-details}

\begin{table}[!t]
	\caption{Logistic regression datasets}
	\centering
	\resizebox{0.85\columnwidth}{!}{%
		\begin{tabular}{lrrrrr}
			\hline
			Dataset      &   $d$ &     $N\textsubscript{train}$ &   $N\textsubscript{test}$ &   $\#$Pos. test data 
			\\
			\hline
			\MYhref{http://archive.ics.uci.edu/ml/datasets/Adult}{\textsc{Adult}}~\citep{adult}        &  10 &  30,162 &    7,413 &             3,700  \\
			\MYhref{https://archive.ics.uci.edu/ml/datasets/Phishing+Websites}{\textsc{Phishing}}~\citep{uci}      & 10 & 8,844 &   2,210 &             1,230  \\
			\MYhref{https://www.cc.gatech.edu/projects/doi/WebbSpamCorpus.html}{\textsc{WebSpam}}~\citep{webspam}      & 127 & 126,185 &   13,789 &             6,907  \\
			\MYhref{https://archive.ics.uci.edu/ml/datasets/diabetes+130-us+hospitals+for+years+1999-2008}{\textsc{HospitalReadmissions}}~\citep{diabetes}      & 10 & 55,163 &   6,079 &             3,044  \\
			\hline
		\end{tabular}
	}
	\label{table:logreg-data}
\end{table}
\begin{table}[!t]
	\caption{Neural linear regression datasets}
	\centering
	\resizebox{0.5\columnwidth}{!}{%
		\begin{tabular}{lrrrrr}
			\hline
			Dataset      &   $d$ &      $N\textsubscript{train}$  &   $N\textsubscript{test}$   \\
			\hline
			\MYhref{https://archive.ics.uci.edu/ml/machine-learning-databases/housing/}{\textsc{Housing}}~\citep{uci}  &  13 &  446 &    50  \\
			\MYhref{https://archive.ics.uci.edu/ml/datasets/YearPredictionMSD}{\textsc{Songs}}~\citep{uci}        &  90 &  463,711 &    51,534\\
			\hline
		\end{tabular}
	}
	\label{table:neurlinreg-data}
\end{table}

The benchmark datasets used in logistic regression (including subpopulations' selection) and neural linear regression experiments are detailed in Tables~\ref{table:logreg-data} and \ref{table:neurlinreg-data} respectively%\footnote{The original versions of all used datasets can be accessed by following the corresponding hyperlinks in the Tables appearing in the electronic version of the thesis.}
, and include: 
\begin{itemize}
	\item a dataset used to predict whether a citizen's income exceeds $50K \$$ per year extracted from USA 1994 census data~(\textsc{Adult}),
	\item a dataset containing webpages features and a label categorizing them as phishing or not~(\textsc{Phishing}),
	\item a corpus of webpages crawled from links found in spam emails~(\textsc{WebSpam}),
	\item a set of hospitalization records for binary prediction of readmission pertaining to diabetes patients~(\textsc{HospitalReadmissions}),
	\item a set of various features from homes in the suburbs of Boston, Massachussets used to model housing price~(\textsc{Housing}), and
	\item a dataset used to predict the release year of songs from associated audio features ~(\textsc{Songs}).
\end{itemize} 

For \textsc{Adult}, \textsc{Phishing} and \textsc{HospitalReadmissions} we fit our statistical models on the first 10 principal components of the datasets, while  all logistic regression benchmark datasets are evaluated on balanced subsets of the test data between the two classes~(see~\cref{table:logreg-data}).

Original versions of the six benchmark datasets were respectively downloaded from the following URLs:~\href{http://archive.ics.uci.edu/ml/datasets/Adult}{http://archive.ics.uci.edu/ml/datasets/Adult},  \href{https://archive.ics.uci.edu/ml/datasets/Phishing+Websites}{https://archive.ics.uci.edu/ ml/datasets/Phishing+Websites}, \href{https://www.cc.gatech.edu/projects/doi/WebbSpamCorpus.html}{https://www.cc.gatech.edu/projects/doi/WebbSpam Corpus.html}, \href{https://archive.ics.uci.edu/ml/datasets/diabetes+130-us+hospitals+for+years+1999-2008}{https://archive.ics.uci.edu/ml/datasets/diabetes+130-us+hospitals+for+ years+1999-2008}, \href{https://archive.ics.uci.edu/ml/machine-learning-databases/housing}{https://archive.ics.uci.edu/ml/machine-learning-databases/housing}, and \href{https://archive.ics.uci.edu/ml/datasets/YearPredictionMSD}{https://archive.ics.uci.edu/ml/datasets/YearPredictionMSD}. 

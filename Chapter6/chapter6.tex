\chapter{Conclusions}
\label{chap:chap6}
\renewcommand*{\MyPath}{../Chapter6}%

In this thesis, we have presented three original pieces of work drawing on one of the fundamental research problems in large-scale machine learning: \emph{finding scalable dataset reductions under constraints commonly arising in real-world data analysis applications}. Our premise has been that principled dataset summarization methods can be harsenessed to enable efficient approximations for the purposes of large-scale data analysis without compromising requirements of privacy and robustness. In this section, we briefly recap our key contributions and suggest directions for future research.

\section{Summary}
\label{sec:summary}


\subsection{Privacy Loss of Coarsened Structured Data}
\label{subsec:ch3-summary}
Reducing the information content and removing explicit identifiers from sensitive datasets prior to public release offers an illusion of privacy. In~\cref{chap:chap3}, we examined a large collection of longitudinal mobility traces recorded by smartphone devices. We converted each pseudonymised user trace record to a truncated graph, which retained the transition patterns among user's most frequent locations, and generated such representations over two different time windows spanning the entire period of tracking. Computing structural similarities via graph kernels allowed us to delevop a linkage attack that was able to reidentify the anonymised mobility graphs at a $3.5\times$ higher success rate compared to random guessing. Our finding stressed that pseudonymisation and coarsening of data cannot protect data subjects against adversaries with access to the infomation of (nearly uniquely) identifying substructures---hence, further elaborating on data reduction techniques that adhere to formal privacy guarantees is required.

\subsection{Privacy-Preserving Bayesian Coresets in High-dimensions}
\label{subsec:ch4-summary}
In~\cref{chap:chap4}, we developed a novel construction for Bayesian coresets. We extended the existing sparse variational inference framework by specifying a richer family of scalable posterior approximations which, instead of points of the original dataset, make use of learnable pseudodata that as variational parameters optimized to summarize the full data likelihood. Our variational approximation enabled effective summarization that is not limited by data dimensionality, unlike previous constructions. Moreover, our coreset construction is amenable both to an incremental, as well as a batch black-box optimization scheme, offering computational time savings compared to state-of-the-art sparse VI methods. Finally, the use of synthetic data, combined with the subsampled Gaussian mechanism, allowed us to yield differentially private dataset summarization. We demonstrated applications of inference over a diverse set of Bayesian models, including Gaussian mean estimation, linear and logistic regression, showing the advantages in data posterior approximation offered by our approach.

\subsection{Robust Bayesian Coresets under Misspecification}
\label{subsec:ch4-summary}
In~\cref{chap:chap5}, we designed a Bayesian coreset construction suitable for summarizing datasets that potentially depart from our statistical model assumptions---as often can be the case in practice, due to containing outliers, and/or being subjected to contamination. We proposed an incremental scheme that attains a sparse approximation of a robust pseudo-Bayesian posterior defined via the~\bdiv, while discerning and retaining a small part of the data inliers instead of the full dataset. Further to offering scalability and reducing data redundancy, our scheme provided a unified and highly-automated solution to the important question of detecting and removing harmful datapoints prior to inference. We evaluated our technique on clean and contaminated data over a range of applications, including Gaussian mean inference, Bayesian linear regression, neural linear regression, and selection of informative data subpopulation combinations, demonstrating reliable posteriors and predictive performance in all examined test cases.


\section{Future Research Directions}
\label{sec:future-research-directions}
The summarization frameworks presented in this dissertation allow numerous probabilistic models to be tractably and reliably deployed in practice. Yet they allude to a realm of so far unexplored research questions, some of which we overiew in the remainder of this section, thus concluding the thesis.

\subsection{Coresets for Models with Structured Likelihoods}
\label{subsec:structure-liks}

Our variational formulations for coreset construction~\cref{eq:pseudo-coreset-vi,eq:coreset-vi} use the assumption that the data likelihood function gets factorised as a product of individual datapoint potentials. To the best of our knowledge, the idea of constructing coresets has not been used yet for inference on models with structured likelihood functions, including time-series and point processes. Recent results on parameter estimation for Hawkes processes using uniform downsampling~\citep{li19} indicate important improvements in efficiency when learning in massive temporal event sequences, even without explicitly optimizing for redundancy in the extracted data subsets.

\subsection{Implicit Differential Privacy Amplification of Data-dependent Compressions}
\label{subsec:implicit-dp-amplification}

In~\cref{chap:chap4} we presented an optimization scheme that yields Bayesian coreset constructions under explicit differential privacy quarantees. As discussed in~\cref{chap:chap2}, a known result in DP literature is that incorporating random sampling in data analysis has implicit privacy amplification effects, \ie~that an algorithm has higher privacy guarantees when run on a random subset of the datapoints instead of the full dataset~\citep{li12, beimel13, bassily14, abadi16}. More recently, Balle \etal~\citep{balle18} presented a unifying methodology that utilises couplings and divergences to reason about DP amplification effects of several random sampling methods~(including Poisson subsampling and sampling with/without replacement), under different data neighbouring relations.

Existing research makes a common assumption that simplifies privacy analysis, but is violated in the case of coresets: the sampling distribution is data-independent. It remains an open-question whether similar approaches can be used to argue about implicit DP amplification when replacing a privacy-sensitive dataset with a coreset---in primitive schemes, coreset construction simply takes the form of importance sampling~\citep{bachem17}. Investigating DP amplification under data-dependent sampling is a direction with far-reaching implications that can contribute to tighter privacy analysis, not only in the case of coresets, but more broadly in all machine learning applications involving importance sampling, which is already a corenestone of many state-of-the-art stochastic learning methods. 


\subsection{Human-oriented Summaries for Scalable Inference}
\label{subsec:human-oriented-pseudodata}

In~\cref{chap:chap4} we presented a method utilising learnable batches of pseudodata to summarize a much larger dataset. Naturally this coreset construction bears the potential of reducing the interpretability of learned pseudodata, since pseudodata are now not a subset of the original dataset but rather the result of a likelihood-specific optimization routine. To remedy related concerns, further interpretability constraints can be explicitly incorporated in the optimization formulation of pseudocoreset variational inference of~\cref{eq:pseudo-coreset-vi}.

Beyond the quest of interpretability, additional research is required on examining other desiderata in human-oriented inference. To name a few, deletion-robustness is often sought or imposed on methods for large-scale data analysis~\citep{mirzasoleiman17, ginart19}: user's \emph{right-to-be-forgotten} is related to imposing bounds on the effects of removing an individual datapoint from an existing dataset, and can be approximately satisfied under differential privacy. Moreover, group \emph{fairness} is one more topic that necessitates further investigation: without special treatment, reducing datasets will potentially transfer existing inequalities across groups in the derived summary, hence a different construction may be sought when aiming to ameliorate unfairness in scalable inference.  

\subsection{Compressing Datasets for Meta-Learning}
\label{subsec:metacoresets}

A distinguighing feature of human intelligence is the ability to adaptively learn new tasks on the basis of prior acquired experience, rather than learning each new task from scratch. Although Bayesian coresets have been originally proposed as an approach for efficient model-specific inference, it seems reasonable to inquire whether sparse dataset summaries can be also useful in meta-learning,~\ie~settings where we aim to learn over a variety of tasks using few training examples per task. Recent work has shown that model-agnostic meta-learning~\citep{finn17} admits reformulations as a hierarchical Bayesian model, and gets perormance improvements via expressing uncertainty~\citep{grant18,finn18}. Apart from offering another avenue for scalability, extracting versatile summaries from a universe of data domains simulates more closely the situtations that a human faces when organizing knowledge for learning in the real world; hence, designing coresets in this context could contribute novel insights into the nature of general intelligence.

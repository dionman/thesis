\chapter{Conclusions}
\label{chap:chap6}
\renewcommand*{\MyPath}{../Chapter6}%

In this thesis, we have presented three original pieces of work drawing on one of the fundamental research problems in large-scale machine learning: \emph{finding scalable dataset reductions under constraints commonly arising in real-world data-analysis applications}. Our premise has been that principled dataset summarization methods can be harsenessed to enable efficient approximations for the purposes of large-scale data analysis without compromising requirements of privacy and robustness. In this section, we briefly recap our key contributions and suggest directions for future research.

\section{Summary}
\label{sec:summary}


\subsection{Privacy Loss of Coarsened Structured Data}
\label{subsec:ch3-summary}
Reducing the information content and removing identifiers from sensitive datasets prior to public release offers an illusion of privacy. In~\cref{chap:chap3} we examined a large collection of longitudinal mobility traces recorded by smartphone devices. We converted each pseudonymised user trace record to a truncated graph, which retained transition patterns among user's most frequent locations, and generated such representations over two different time windows across the period of tracking. Computing structural similarities via graph kernels allowed us to delevop a linkage attack that was able to reidentify the anonymised mobility graphs at a $3.5\times$ higher success rate compared to random guessing. Our finding stressed that pseudonymisation and coarsening of data cannot protect data subjects against adversaries with access to the infomation of (nearly uniquely) identifying substructures---hence, further elaborating on data reduction techniques that adhere to formal privacy guarantees is required.

\subsection{Privacy-Preserving Bayesian Coresets in High-dimensions}
\label{subsec:ch4-summary}
In~\cref{chap:chap4} we developed a novel construction for Bayesian coresets. We extended the existing sparse variational inference framework by specifying a richer family of scalable posterior approximations which, instead of points of the original dataset, make use of learnable pseudodata that are able to summarize the full data likelihood. Our variational approximation enabled effective summarization that is not limited by data dimensionality. Moreover, our coreset construction is amenable both to an incremental, as well as a batch black-box optimization scheme, offering computational time savings compared to state-of-the-art sparse VI methods. Finally, the use of synthetic data, combined with the subsampled Gaussian mechanism, allowed us to yield differentially private dataset summarization. We demonstrated applications of inference over a diverse set of Bayesian models, including Gaussian mean estimation, linear and logistic regression, showing the advantages in data posterior approximation offered by our approach.

\subsection{Robust Bayesian Coresets under Misspecification}
\label{subsec:ch4-summary}
In~\cref{chap:chap5} we designed a Bayesian coresets construction suitable for summarizing datasets that depart from our statistical assumptions, due to containing outliers, and/or being subjected to contamination. We proposed an incremental scheme for attaining a sparse variational posterior, that can scalably approximate robust pseudo-Bayesian posteriors, computed on the full data via the~\bdiv. Further to reducing data redundancy, our scheme offered a unified and highly-automated solution to the important question of detecting and removing harmful datapoints prior to inference. We evaluated our technique on clean and contaminated data over a range of applications, including Gaussian mean inference, Bayesian linear regression, neural linear regression, and selection of informative data subpopulation combinations, demonstrating reliable posteriors and predictive performance in all examined test cases.


\section{Future Research Directions}
\label{sec:future-research-directions}
The summarization frameworks presented in this dissertation allow numerous probabilistic models to be tractably and reliably deployed in practice. Yet they allude to a realm of so far unexplored research questions, some of which we overiew in the remainder of this section, thus concluding the thesis.

\subsection{Coresets for Models with Structured Likelihoods}
\label{subsec:structure-liks}

Our variational formulations for coreset construction~\cref{eq:pseudo-coreset-vi,eq:coreset-vi} use the assumption that the data likelihood function gets factorised as a product of individual datapoint potentials. To the best of our knowledge the idea of constructing coresets has yet been pursued in models with structured likelihood functions, including time-series and point processes. Recent results on parameter estimation for Hawkes processes using uniform downsampling~\citep{li19} indicate important improvements in efficiency when learning in massive temporal event sequences, even without optimizing for redundancy in the extracted dataset samples.

\subsection{Implicit Differential Privacy Amplification of Data-dependent Compressions}
\label{subsec:implicit-dp-amplification}

In~\cref{chap:chap4} we presented an optimization scheme that yields Bayesian coreset constructions under explicit differential privacy quarantees. A known result in DP literature is that incorporating random sampling in data analysis has implicit privacy amplification effects, \ie~that an algorithm has higher privacy guarantees when run on a random subset of the datapoints instead of the full dataset~\citep{li12, beimel13, bassily14, abadi16}. More recently, Balle \etal~\citep{balle18} presented a unifying methodology that utilises couplings and divergences to reason about DP amplification effects of several random sampling methods~(including Poisson subsampling and sampling with/without replacement), under different data neighbouring relations. Existing research makes a common assumption that simplifies privacy analysis, but is violated in the case of coresets: the sampling distribution is data-independent. It remains an open-question whether similar approaches can be used to argue about implicit DP amplification when replacing a privacy-sensitive dataset with a coreset---in primitive schemes, coreset construction simply takes the form of importance sampling~\citep{bachem17}. Investigating DP amplification under data-dependent sampling is a direction with far-reaching implications that can contribute to tighter privacy analysis not only in the case of coresets, but more broadly in all machine learning applications involving importance sampling, which is already a corenestone of many state-of-the-art stochastic learning methods. 


\subsection{Human-oriented Summaries for Scalable Inference}
\label{subsec:human-oriented-pseudodata}

In~\cref{chap:chap4} we presented a method utilising learnable batches of pseudodata to summarize a much larger dataset. Naturally this coreset construction bears the potential of reducing the interpretability of learned pseudodata, since summarizing datapoints are now not a subset of the original dataset but rather the result of a model-specific optimization routine. To remedy potential interpretability issues, further interpretability constraints can be explicitly incorporated in the optimization formulation of pseudocoreset variational inference of~\cref{eq:pseudo-coreset-vi}.

Beyond the quest of interpretability, additional research is required on examining other desiderata in human-oriented inference. To name a few, deletion-robustness is often sought or imposed on methods for large-scale data analysis~\citep{mirzasoleiman17, ginart19}: user's \emph{right-to-be-forgotten} is related to imposing bounds on the effects of removing an individual datapoint from an existing dataset and can be approximately satisfied under differential privacy. Moreover, group \emph{fairness} is one more topic that necessitates further investigation: without special treatment, reducing datasets will potentially transfer existing inequalities across groups in the extracted summary, hence a different construction may be sought when aiming to ameliorate unfairness in scalable inference.  

\subsection{Coresets for Meta-Learning}
\label{subsec:metacoresets}
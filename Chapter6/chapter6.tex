\chapter{Conclusions}
\label{chap:chap6}
\renewcommand*{\MyPath}{../Chapter6}%

In this thesis, we have presented three original pieces of work drawing on the importance of dataset reductions for learning in the massive data scale: Our premise has been that principled dataset summarization methods can be harsenessed to enable scalable approximations for the purposes of inference on large data without compromising privacy and robustness of learning.
In this section, we briefly summarize our key contributions and suggest directions for future research.

\section{Summary}
\label{sec:summary}

\subsection{Privacy Loss of Coarsified Structured Data}
\label{subsec:ch3-summary}


\subsection{Privacy-Preserving Bayesian Coresets in High-dimensions}
\label{subsec:ch4-summary}

\subsection{Robust Bayesian Coresets on Contaminated Datasets}
\label{subsec:ch4-summary}


\section{Future Research Directions}
\label{sec:future-research-directions}
The results presented above allude to several yet unexplored research questions, some of which we overiew in this section, thus concluding the thesis.

\subsection{Bayesian Coresets for Models with Structured Likelihoods}
\label{subsec:structure-liks}

\subsection{Implicit Differential Privacy Amplification of Data-dependent Summarizations}
\label{subsec:implicit-dp-amplification}

In~\cref{chap:chap4} we presented a scheme that achieves  Bayesian coresets constructions under specified differential privacy quarantees. Recent research has demonstrated that incorporating random sampling in data analysis has implicit privacy amplification effects. Moreover,~\citep{balle18} presented a general methodology utilising couplings and divergences can be used to reason about DP amplification effects of several sampling techniques~(including random and Poisson subsampling) under different data neighbouring relations. Existing research has a common assumption that simplifies privacy analysis, but is violated in the case of coresets: the sampling distribution is data-independent. It remains an open-question whether similar methods can be used to argue about implicit DP amplification when replacing a privacy-sensitive dataset with a non-private coreset. Investigating DP amplification under data-dependent sampling is an interesting direction that can lead to tighter privacy loss quantification not only in the case of coresets, but more broadly in all machine learning applications involving importance sampling, which is already a corenestone of many stochastic learning methods. 


\subsection{Human-oriented Summaries for Scalable Inference}
\label{subsec:human-oriented-pseudodata}

In~\cref{chap:chap4} we presented a method utilising learnable batches of pseudodata that summarize a much larger dataset. Naturally this coreset construction bears the potential of reducing the interpretability of learned pseudodata. However, interpretability constraints can be explicitly incorporated in the optimization formulation of pseudocoreset variational inference of~\cref{eq:coreset-vi}.

Beyond the quest of interpretability, further research is required for examining other desiderata in human-oriented inference. To name a few, deletion-robustness is often sought or imposed on methods for large-scale data analysis: the \emph{right-to-be-forgotten} is related to imposing bounds on  the effects of removing an individual datapoint from an existing dataset. Moreover, group \emph{fairness} is one more topic that necessitates further investigation: without special treatment, reducing datasets will potentially transfer existing inequalities between groups in the summary, hence a different construction should be sought when aiming to ameliorate unfairness in scalable learning.  
\section{Summary \& discussion}% \& further directions}
\label{sec:conclusion}
In this chapter, we proposed a general purpose framework for yielding  contamination-robust summarizations of massive scale datasets for inference. Relying on recent advances in Bayesian coresets and robustified approximate inference under the \bdiv{}, we developed a greedy black-box construction that efficiently shrinks big data via keeping informative datapoints, while simultaneously rejecting outliers.
Finally, we presented experiments involving various statistical models, and simulated and real-world datasets, demonstrating that our methodology outperforms existing techniques in scenarios of structured and unstructured data corruption. 

Further directions include developing more methods for adaptive tuning of the robustness hyperparameter $\beta$, as well as applying our techniques to more complicated statistical models, including ones with structured likelihood functions (e.g. time-series and temporal point processes). Moreover, future experimentation may consider stronger adversarial settings where summaries are initialized to data subsets that already contain outliers. 
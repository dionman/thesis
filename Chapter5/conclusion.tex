\section{Conclusion \& further directions}
\label{sec:conclusion}
In this work, we proposed a general purpose framework for yielding  contamination-robust summarizations of massive scale datasets for inference. Relying on recent advances in Bayesian coresets and robustified inference under the \bdiv{}, we developed a greedy black-box construction that efficiently shrinks big data via keeping informative datapoints, while simultaneously rejecting outliers.
 Finally, we presented experiments involving various statistical models, and simulated and real-world datasets, demonstrating that our methodology outperforms existing techniques in scenarios of structured and unstructured data corruption. 

Our future work will be concerned with considering stronger adversarial settings where summaries are initialized to data subsets that already contain outliers. Further directions also include automating the tuning of the robustness hyperparameter $\beta$, as well as applying our techniques to more complicated statistical models, including ones with structured likelihood functions (e.g. time-series and temporal point processes).
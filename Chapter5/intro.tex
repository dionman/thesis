\section{Introduction}
\label{sec:introduction}

Machine learning systems perpetually collect growing datasets, such as product reviews, posting activity on social media, users feedback on services, or insurance claims. The rich information content of such datasets has opened up an exciting potential to remedy various practical problems. Hence, recent years have witnessed a surge of interest in scaling up inference in the large-data regime via stochastic and batch methods~\cite{angelino16, hoffman13, welling11}. Most of related approaches have treated datapoints indiscriminantly; nevertheless, it is well known that not all datapoints contribute equally valuable information for a given target task~\cite{ghorbani19}. 

Datasets collected in modern applications contain redundant input samples that reflect very similar statistical patterns, or multiple copies of identical observations. Often input aggregates subpopulations emanating from different distributions~\cite{zheng08, zhuang15}. Moreover, the presence of outliers is a ubiquitous challenge, attributed to multiple causes. In the first place, noise is inherent in most real-world data collection procedures, creating systematic outliers: crowdsourcing is prone to mislabeling~\cite{frenay13} and necessitates laborious data cleansing~\cite{lewis04, paschou10}, while measurements commonly capture sensing errors and system failures. Secondly, outliers can be generated intentionally from information contributing parties, who aim to compromise the functionality of the application through data poisoning attacks~\cite{barreno10, biggio12, li16, koh17, steinhardt17, ghorbani19}, realised for example via data generation from fake accounts. Outliers detection is  challenging, particularly in high dimensions~\cite{diakonikolas19, dickens20}. Proposed solutions \mbox{often are} model-specific, and include dedicated learning components which increase the time complexity of the application, involve extensive hyperparameter tuning, introduce data redundancies, or require model retraining~\cite{sheng08, whitehill09, raykar10, karger11, liu12, zhang16}. On the other hand, operating on a corrupted dataset is brittle, and can decisively degrade the predictive performance of downstream statistical tasks, deceptively underestimate model uncertainty and lead to incorrect decisions. 

In this work, we design an integrated approach for inference on massive scale observations that can jointly address scalability and data cleansing for complex Bayesian models, via robust data summarization. Our method inherits the full set of benefits of Bayesian inference and works for any model with tractable likelihood function. At the same time, it maintains a high degree of automation with no need for manual data inspection, no additional computational overhead due to robustification, and can tolerate a non-constant number of corruptions. Moreover, our work points to a more efficient practice in large-scale data acquisition, filtering away less valuable samples, and indicating the regions of the data space that are most beneficial for our inference task. 

Our solution can be regarded as an extension of Bayesian coreset methods that can encompass robustified inference. Bayesian coresets~\cite{huggins16, campbell19jmlr, campbell19neurips} have been recently proposed as a method that enables Bayesian learning at scale via substituting the complete dataset over inference with an informative sparse subset thereof. Robustified Bayesian inference methods~\cite{berger94} have sought solutions to mismatches between available observations and the assumed data generating model, %via generalizing the data likelihood function, 
via proposing heavy-tailed data likelihood functions~\cite{huber09, insua12} and localization~\cite{definetti61, wang18}, using robust statistical divergences~\cite{futami18, knoblauch18, miller19},  or inferring datapoints-specific importance weights~\cite{wang17}. Here, we cast coreset construction in the framework of robustified inference, introducing \emph{\bcores{}}, a method that learns sparse variational approximations of the full data posterior under the \bdiv{}. In this way, we are able to yield summaries of large data that are distilled from outliers, or data subpopulations departing from our statistical model assumptions. Importantly, \bcores{} can act as a preprocessing step, and the learned data summaries can subsequently be given as input to any ordinary or robustified black-box inference algorithm.

The rest of this chapter is organized as follows. In \cref{sec:preliminaries,sec:method} we introduce necessary concepts from Bayesian inference, and present our proposed method. In \cref{sec:evaluation} we expose experimental results on simulated and real-world benchmark datasets: we consider diverse statistical models and scenarios of extensive data contamination, and demonstrate that, in contrast to existing summarization algorithms, our method is able to maintain reliable predictive performance in the presence of structured and unstructured outliers. Finally, in \cref{sec:conclusion} we provide conclusions and discuss future works.

This chapter is based on~\citep{beta-cores}.





\chapter{Background Material}
\label{chap:chap2}

This chapter aims to set the context for the remainder of this thesis. Various concepts pertaining to this thesis, including Bayesian inference, exponential family distributions and differential privacy, are briefly introduced in the following.

\section{Comparing Probability Distributions}
\label{subsec:b-divergences}
A critical component in constructing and evaluating inference algorithms is having a divergence measure, that captures informatively how similar two probabilities are. Statistical divergences are relaxations of distance measures, that are always non-negative and equal zero iff their arguments are identical---albeit without the requirements of being symmetric in their arguments, or satisfying the triangle inequality. The most commonly used divergence measure in approximate inference---which will directly serve as the objective quantifying the quality of our summaries in ~\cref{chap:chap4}---is the \emph{Kullback-Leibler}~(KL) divergence, also named \emph{relative entropy}~\citep{kullback51,kullback59}. For continuous densities $\pi_1(\theta)$ and $\pi_2(\theta)$, KL divergence is defined as 
\[
\label{eq:bkl-def}
\kl{\pi_1(\theta)}{\pi_2(\theta)} \defined \int \pi_1(\theta) \log\frac{\pi_1(\theta)}{\pi_2(\theta)} d\theta.
\]
In particular, for two Gaussian distributions $\mcN_1(\mu_1, \Sigma_1)$ and $\mcN_1(\mu_1, \Sigma_1)$ the KL divergence is computable in closed form as follows
\[
\kl{\mcN_1}{\mcN_2} = \frac{1}{2}\left[\tr(\Sigma_2^{-1}\Sigma_1) - (\mu_1-\mu_2)^T\Sigma_2^{-1}(\mu_1-\mu_2) - M + \log\frac{|\Sigma_2|}{|\Sigma_1|}\right].
\]

%\section{Robust Inference}
%\label{sec:b-robust-inference}
When working in data setups that are likely to be contaminated by outliers, we get substantial performance improvements when enhancing our algorithms with \emph{robustness}. Relying on KL divergence cannot sufficiently address this concern. A robustified divergence, termed \bdiv{}, was instead proposed in~\citep{basu98,eguchi01}, that is able to downweight outlying data. Considering again the densities $\pi_1, \pi_2$, \bdiv{} is defined as 
\[
\betadistance{\pi_1}{\pi_2} \defined \frac{1}{\beta} \int\left(\pi_1^\beta(\theta) - \pi_2^\beta(\theta)\right)\pi_1(\theta)d\theta - \frac{1}{\beta+1}\int\left(\pi_1^{\beta+1}(\theta) - \pi_2^{\beta+1}(\theta)\right)d\theta.
\] 
One can easily show that \bdiv{} converges to KL divergence when $\beta \rightarrow 0$.

\section{Exponential Families}
\label{sec:b-expfam}

The exponential family~\citep{wainwright08} is a broad class of probability distributions, sharing a set of important properties that facilitate tractable inference. Exponential family members include numerous well-known distributions, such as the Poisson distribution, the Gamma distribution, and the Gaussian or normal distribution. 

\begin{ndefn}[Exponential family] \label{def:bexpfam}
	A collection of densities $\pi$, with respect to a base measure $\nu$ indexed by a vector of parameters $\theta$, is an \emph{exponential family} of densities if it can be written as
	\[
	\pi_{\theta}(x) = h(x) \exp\left( \langle \theta, t(x) \rangle - Z(\theta) \right).
	\label{eq:bexpfam}
	\]
	We call $t(x): \mcX \rightarrow \reals^{d}$ the \emph{sufficient statistics} of the data, $h(x)$ the \emph{base density} and 
	\[
	Z(\theta)\defined \log \int e^{\langle \theta, t(x) \rangle } h(x) \nu(dx)
	\label{eq:b-logpartition}
	\]
	the \emph{log-partition function}.
\end{ndefn}

The parameters space of interest, referred to as the \emph{natural parameter space}, is the space $\Omega \subseteq \reals^d $ that contains all $\theta$ such that $Z(\theta)$ is finite. We say a family is \emph{regular} if $\Omega$ is open.

An important property of exponential family densities is that the derivatives of the log-partition function $Z$ are related to the moments of the sufficient statistics as follows.

\begin{nprop}[Derivatives of the log-partition function via expected statistics] \label{prop:bgradZ}
	For a regular exponential family of densities in the form of~\cref{eq:bexpfam}, the log-partition function has derivatives of all orders on its domain $\Omega$, while for the first two derivatives hold the following
	\[
	\grad Z(\theta) = \EE_{\theta}[t(x)]
	\label{eq:bgradZ}
	\] 	
	and 
	\[
	\grad^2Z(\theta) = \cov_{\theta}[t(x)] \defined \EE_{\theta}[t(x)t(x)^T] - \EE_{\theta}[t(x)]\EE_{\theta}[t(x)]^T.
	\label{eq:bhessZ}
	\]
\end{nprop}

\cref{prop:bgradZ} allows efficient approximations for the gradient and Hessian of $ Z $ using empirical estimates of the first two moments of the sufficient statistic, which we take advantage of in the variational approximation schemes to be introduced in~\cref{chap:chap4,chap:chap5}.

\section{Probabilistic Learning at  a Glance}
\label{sec:b-bayesian-inference}

Bayesian probabilistic modeling provides a principled framework for  learning from observed data, incorporating expert knowledge, handling model uncertainty and drawing coherent inferences in a unified way, following the languange of probability theory.

In (parametric) Bayesian learning settings we are generally given a set of observations $x = \{x_1,...,x_N\} \subseteq \mcX$, and aim to find a vector $\theta $ of latent random variables parameterising an assumed probabilistic model that \emph{is likely to} explain them. In the Bayesian paradigm, we first assume a \emph{prior} distribution over the parameters $\pi_0(\theta)$, that encodes our beliefs about the uncertainty in $\theta$ before observing any data. Once the data are taken into account, our beliefs shoud be updated accordingly, in order to better describe the observed distribution. For this purpose a \emph{likelihood} distribution $\pi(x|\theta)$ needs to be defined; the likelihood quantifies the probability of the observations under the assumed statistical model for parameters set to $\theta$. Combining the above distributions we are ready to formulate \emph{Bayes' theorem}, the fundamental rule which gives the \emph{posterior} beliefs for our parameters updated in light of the observed data
\[
\pi(\theta|x) = \frac{\pi(x|\theta)\pi_0(\theta)}{\pi(x)}.
\label{eq:bbayes-rule}
\] 
Henceforth any quantity of interest $g(\cdot)$ involving the latent variables $\theta$ is calculated using expectations under the posterior---which is considered to be the complete information about $\theta$ given the data $x$---as follows
\[
\EE_{\theta \sim \pi(\theta|x)}\left[g(\theta)\right] \defined \int g(\theta) \pi(\theta|x) d\theta.
\label{eq:binference}
\]
Computing~\cref{eq:binference} is known as doing \emph{inference} on our statistical model.

A key challenge in computing the posterior according to~\cref{eq:bbayes-rule} is evaluating the normalizer, called \emph{marginal likelihood} (or \emph{model evidence}), which in a continuous parametric space takes the form
\[
\pi(x) = \int \pi(x|\theta) \pi_0(\theta) d\theta.
\label{eq:bmarginal-likelihood}
\]
\emph{Marginalising}, \ie~computing the integral of~\cref{eq:bmarginal-likelihood}, can be done using analytical tools for a number of simple Bayesian models---some of which will be discussed in the remainder, including Gaussian mean inference, Bayesian and neural linear regression---where the likelihood is conjugate to the prior. However, for the vast majority of interesting statistical models marginalization cannot be done in closed form and should be approximated instead. Aiming to address such cases, approximate Bayesian inference has emerged as an active research area for many decades. In the remainder of the section we present an overview of existing approaches addressing approximate inference that are relevant to our algorithms. For a more detailed exposure, including methods beyond the scope of this thesis (e.g. expectation propagation), cf.~\citep{bishop06,murphy12,angelino16}.

\subsection{Laplace's Method}
\label{subsec:b-laplace-method}

Point estimates of $\theta$, obtained for example via \emph{maximum a posteriori~(MAP)} or \emph{maximum likelihood estimation~(MLE)}, are cheap to compute, as they correspond to solutions of optimization problems involving only the unnormalised rhs of~\cref{eq:bbayes-rule}---on the other hand, they cannot capture the uncertainty of our posterior beliefs. Laplace's method~\citep{mackay03} is an approximate inference scheme that makes a first step towards uncertainty awareness, offering a non-degenerate, yet inexpensive to compute, approximate posterior for $\theta$.

Let us write the posterior of~\cref{eq:bbayes-rule} in the following equivalent form
\[
\pi(\theta|x) = \frac{1}{Z} e^{-E(\theta)},
\]
where $E(\theta) \defined - \log \pi(\theta, x)$ is called the \emph{energy function}, and $ Z $ is the unknown normalization constant. Taking the Taylor series expansion of $\theta$ (up to order 2) around the mode $ {\htheta \defined \arg \underset{\theta}{\min} \;E(\theta)}$, we obtain the approximation 
$ \hpi(\theta, x) = e^{-E(\htheta)}\exp\left((\theta - \htheta)^T\Lambda (\theta - \htheta)\right)$ where  $ \Lambda \defined -\grad^2 E(\theta)\Big|_{\theta = \htheta}$.
Hence we have
\[
\pi(\theta|X) \approx \frac{1}{Z}   \hpi(\theta, x)  \propto \mcN(\htheta, \Lambda^{-1}), 
\label{eq:blaplace-approx}
\]  
\ie~the posterior can be approximated by a (unimodal) Gaussian, where the mean corresponds to the minimum of the energy function and the covariance is the negative Hessian of the energy function evaluated on the mean. Clearly, using standard numerical optimization routines, e.g. quasi-Newton methods, we can achieve fast convergence to $\htheta$. 

Laplace approximations will be used as coarse posterior approximations over our coreset summary constructions.

\subsection{Sampling Methods}
\label{subsec:b-sampling-methods}
In the absence of analytical formulae, integrals in the form of~\cref{eq:binference} can be approximated via empirical averaging, using samples from the target posterior distribution
\[
 \int g(\theta) \pi(\theta|x) d\theta \approx \frac{1}{S} \sum_{s=1}^{S} g(\theta_s), \quad (\theta_s)_{s=1}^{S} \distiid \pi(\theta|x) .
\]
Markov Chain Monte Carlo~(MCMC), the workhorse of approximate Bayesian inference, is a framework of established tools that pursue the above idea efficiently~\citep{geyer92, gilks05, robert05}. 

MCMC offers approximations to expectations \wrt intractable probability distributions via simulating an ergodic random walk in the state space of the model which admits the posterior distribution as its stationary distribution. As implied by the strong law of large numbers, the MC estimate---formed using (effectively independent) samples from the stationary distribution---converges to the true expecation almost surely as $s \rightarrow \infty$; this property makes MCMC methods theoretically appealing, as it endows the estimators with strong \emph{asymptotic exactness} guarantees. Moreover, if $g$ is a real function, using the central limit theorem, it can be shown that the standard error of a MC estimator scales asymptotically as $O(\frac{1}{\sqrt{S}})$, independent of the dimension of $\theta$. Differing in the way that the Monte Carlo chain is constructed, as well as the offered level of automation, several methods of MCMC infrence have emerged, including the Metropolis-Hastings~\citep{andrieu03}, the Hamiltonian Monte-Carlo~\citep{neal11}, and the No-U-Turn-Sampler~(NUTS)~\citep{hoffman14}. NUTS will be used as a reference method to evaluate summarization performance in part of our experiments over~\cref{chap:chap4,chap:chap5}.

Bounds on the number of MCMC iterations required until we obtain a satisfactory posterior approximation can hardly be automated, as they are highly problem-specific, and in practice heuristics are used to decide when sampling should stop. Typically each sample requires at least one evaluation of a function proportial to $\pi$, scaling at cost $\Theta(N)$ which becomes a burden in big data applications---on this account, methods operating on data subsets have been proposed, including~\citep{welling11,korattikara14,bardenet14}. Despite these shortcomings, in settings where data are high-dimensional, and likelihood surface lacks strcuture that we could exploit over inference, MCMC remains the gold standard for practitioners.



\subsection{Variational Inference}
\label{subsec:b-variational-inference}
Variational inference~(VI)~\citep{jordan99,blei17} takes a fundamentally different approach to  address approximate inference.
The problem formulation underpinning all VI methods is to find a member $q^*$ within a family of tractable probability distributions $Q$ that most closely matches our true posterior $\pi$ (typically in the KL-sense)
\[
\label{eq:bvi}
q^{*}(\theta; x) \defined \arg \underset{q\in Q}{\min} \kl{q(\theta)}{\pi(\theta|x)}.
\]
In this way, Bayesian posterior inference gets reduced into an optimization problem; hence, techniques allowing scaling up optimization (e.g. random subsampling) can in principle be applied in VI methods, enabling scalable inference of approximate posteriors~\citep{hoffman13}.

In passing, %whilst it is beyond the scope of this introduction to give a detailed presentation of how the problem of~\cref{eq:bvi} is approached, 
we note that, in classical Variational Bayes schemes, expanding the KL divergence according to~\cref{eq:bkl-def} makes the log-evidence appear in the objective 
\[
\kl{q(\theta)}{\pi(\theta|x)} = \EE_{\theta \sim q(\theta)}\left[\log q(\theta)\right] - \EE_{z\sim q(\theta)}\left[\log \pi(x, \theta)\right] + \log \pi(\theta).
\]
Since this term is not a function of $q$, it can be subtracted and the problem is reformulated as minimizing the remaining two terms, the negation of which is known as the \emph{evidence lower bound}~(ELBO).
\[
q^{*}(\theta; x) \defined \arg \underset{q\in Q}{\min} \left(-\text{ELBO}(q, x)\right), \qquad
\text{ELBO}(q, x) \defined \EE_{q}\left[\log \pi(x, \theta)\right] - \EE_q \left[\log q(\theta)\right]
\]
Via Jensen's inequality, ELBO can be shown to be a lower bound of the marginal log-likood of $x$ as expectation \wrt~$q$. As opposed to MCMC methods, theoretical guarantees for inferential results of the solution to~\cref{eq:bvi} can only be obtained for a few simple statistical models for the following main reasons: optimization methods can often converge to bad local optima; also, depending on the statistical divergence and variational family used, VI might return miscalibrated posterior variance estimates~\citep[Chapter~10]{bishop06}.

The simplest family $Q$ that can be used for VI is the \emph{mean-field variational family} which relies on the simplifying assumption of independence among the coordinates of $\theta$, \ie~$q(\theta) \defined \Pi_{d=1}^{D}q_d(\theta_d)$.
Our VI scheme proposes approximations within the \emph{exponential family}, which generally allow less restricted posteriors. Additionally, it can circumvent the use of ELBO, and instead be directly applied on the original variational formulation of~\cref{eq:bvi}, since MC estimates of the gradient of the log-evidence term can be extracted as per~\cref{prop:bgradZ}. 

\subsection{Bayesian Coresets}
\label{subsec:b-coresets}
Owing to their requirement for multiple evaluations of the data (log-)likelihood---a computation scaling at $\Omega(N)$---MCMC and VI methods quickly become prohibitively expensive in the large-data regime. Various stochastic schemes have been proposed to circumvent this computation, evaluating the likelihood on random data minibatches: despite achieving computational savings and often being straightforward to implement, such schemes rarely offer guarantees on posterior approximation quality, and lack a rigorous principle over minibatch selection step, hence retaining part of the redundancy of the full data collection in the extracted samples.

Bayesian coresets~\citep{huggins16,campbell18,campbell19jmlr,campbell19neurips} make the assumption that the full dataset has some degree of inherent redundancy, and put forth the idea of scaling up inference via the application of a preprocessing step where \emph{part of the data gets retained under the criterion of likelihood approximation}. In the spirit of the first coresets proposed in the field of computational geometry, initial construction schemes for coreset-based inference~\citep{huggins16,lucic17training} utilize \emph{importance sampling} according to the datapoints sensitivity, \ie~a non-negative quantity measuring the redundancy of each of the datapoints \wrt the statistical model of interest. Although giving theoretical gurantees for the approximation quality achieved by the coreset, importance sampling based constructions have typically two shortcomings: (\emph{i}) they rely on efficiently computable upper bounds of the sensitivity, and (\emph{ii}) they do not have a sense of a residual posterior error, hence are limited by common MC rates in approximating data likelihood, offering error $\eps = O(\frac{1}{\sqrt{M}})$ for coreset size $M$.

Reformulating coreset construction as sparse function approximation in a Hilbert space~(\emph{Hilbert coresets}), Campbell and Broderick~\citep{campbell18,campbell19jmlr} introduced alternative optimization formulations for the problem. They showed that using inner-product inducing norms can lead to faster incremental construction schemes that, critically, can guide next datapoints selection by the direction of greatest impovement. Moreover, they made use of a coarse posterior approximation and random projections to efficiently compute Hilbert norms that capture the divergence between the coreset and the true posterior, and proposed faster sparse constructions under polytope and hypersphere constraints.

In more recent work, Campbell and Beronov~\citep{campbell19neurips} casted Bayesian coresets to a problem of sparse variational inference within an exponential family, named \emph{Riemannian coresets}. Riemannian coresets raised the requirement for fixing a coarse posterior that appears when computing the norm in practical Hilbert coreset constructions, achieving full automation and improvement of approximation (in KL) over a larger range of summary sizes.


\section{Representing Data}
\label{sec:b-representing-data}
Extracting a relevant feature representation is an important step in the context of statistical pattern recognition. For this purpose a feature map 
\[
\phi: \mcX \rightarrow \mcH,
\]
is sought which transforms the datapoints from the original data space $\{x_n\}_{n=1}^{N}$, $x_n \in \mcX$, into \emph{feature representations} in a Hilbert space $\{\phi(x_n)\}_{n=1}^{N}$, $\phi(x_n) \in \mcH$. Then the patterns of interest can be revealed via applications of inner products in the Hilbert space $\langle A, \phi(x) \rangle_{\mcH}$. There is an extensive literature on constructing data representations; for the purposes of this thesis, in the remainder of the section we focus on two of them: kernel methods and random projections.


\subsection{Kernels}
\label{subsec:b-kernels}

The main tool in kernel methods~\cite{scholkopf02} is the \emph{kernel function} defined below.
\begin{ndefn}[Kernel function] \label{def:bkernelfun}
	A symmetric function $k: \mcX \times \mcX \rightarrow \reals $ is a positive definite kernel function, or kernel, if for all $N>1$, $x_1, \ldots, x_N \in \reals$, and $c_1, \ldots, c_N \in \reals$ 
	\[
	\sum_{i,j=1}^{N} c_ic_j k(x_i, x_j) \geq 0.
	\]
\end{ndefn}
Every kernel is associated with a feature map $\phi$ as follows.
\begin{ndefn}[Kernel representation] \label{def:bkernelrepr}
	 A function $k: \mcX \times \mcX \rightarrow \reals$ is a kernel if and only if there exists a Hilbert space $\mcH$ and a feature map $\phi: \mcX \rightarrow \mcH$ such that for all $x, x' \in \mcX$
	 \[
	 \label{eq:bkernelrep}
	 k(x,x') = \langle \phi(x), \phi(x') \rangle_\mcH.
	 \]
	 Feature map $\phi$ endows each datapoint $x \in \mcX$ with a kernel representation $\phi(x)$.
\end{ndefn}
A kernel representation might be lacking an explicit closed form, but can always be accessed via the inner product of~\cref{eq:bkernelrep}, which is the central object of interest in learning with kernels.

Examples of widely-used kernel functions include
\begin{itemize}
	\item The polynomial kernel $k(x,x') = \left(\langle x, x'\rangle + c\right)^d$.
	\item The Gaussian kernel $k(x,x') = \exp(-\gamma||x-x'||_{2}^2)$.
	\item The Sigmoid kernel $k(x,x') = \tanh(\beta\langle x, x'\rangle + c)$.
\end{itemize}

Kernel methods induce \emph{non-parameteric} representations on the data, \ie~when given a set with $ N $ datapoints of dimension $d$, kernels effectively map each datapoint to an $N$-dimensional representation. %Although allowing representation power scaling with the number of datapoints, such methods often reach prohibitive computational cost.

\subsection{Finite-dimensional Random Projections}
\label{subsec:b-random-features}

Kernel methods appeal to large-scale learning due to their non-parametric nature: their representation power scales with the number of datapoints, hence they can learn complex, highly non-linear structure from the data; however, their time and memory cost scales adversely with the dataset size. Random features~\citep{rahimi08} remedy poor complexity scaling issues via utilising \emph{parametric finite-dimensional} data representations. We motivate this concept via an application arising in Hilbert coreset constructions~\cite{campbell19jmlr}.

Denote by $ f(\theta) \defined \sum_{n=1}^{N} \log\pi(x_n|\theta) $ the log-likelihood function of a dataset $x \defined {(x_n)}_{n=1}^{N}$, and by $ f(\theta,w) \defined \sum_{n=1}^{N} w_n\log\pi(x_n|\theta) $ the corresponding log-likelihood of a Hilbert coreset $\{(w_n, x_n)\}_{n=1}^{N} : ||w||_0 \ll N$ constructed on the data. The quality of posterior approximation that this coreset offers can be quantified by using an $L^2$ norm on the log-likelihoods under a weighting distribution $\hpi$ that has the same support with the posterior $\pi$ 
\[
||f(\theta, w) - f(\theta)||_{\hpi,2} \defined \EE_{\hpi} \left[ (f(\theta) - f(\theta, w))^2\right],
\]
and induced inner product
\[
\label{eq:binner-prod-hc}
\langle f(x_n), f(x_m) \rangle_{\hpi, 2} \defined \EE_{\hpi}\left[f(x_n, \theta), f(x_m, \theta)\right].
\]
The weighting distribution $\hpi$ can be selected from a set of cheap posterior approximations, for example using Laplace's method, or running a few rounds of an MCMC algorithm. In the general case the norm of~\cref{eq:binner-prod-hc} is not available in closed form, hence a random projection can be used instead to approximate it according to the following steps:
\begin{enumerate}
	\item Sample $J$ values for $\theta$ from the weighting distribution $(\htheta_j)_{j=1}^{J} \distiid \hpi$.
	\item For $n=1 \ldots N$ compute a $J$-dimensional projection $ \hf(x_n) \defined \sqrt{\frac{1}{J}}[f(x_n, \htheta_1) \ldots f(x_n, \htheta_J)]$.
\end{enumerate}
In this way we get an unbiased finite-dimensional estimator of the inner products
\[
\langle f(x_n), f(x_m) \rangle_{\hpi, 2}  \approx \hf(x_n)^T \hf(x_m).
\]


\section{Differential Privacy}

Differential privacy~(DP)~\citep{dwork2006calibrating,dwork14} enforces a stability property on randomized algorithms accessing a sensitive database and releasing aggregate information thereof, that mitigates the privacy threat due to changing an individual entry of the database. It limits disclosure of information about individuals within the database, offering strong indistinguishability guarantees regardless of the side information that an adversary might possess. 

DP definition requires a notion of "neighboring" databases.
To define distance between two databases $x, x' \in \mcX$ of size $ N $ we use the Hamming distance
\[
D_{H}(x, x') \defined \#\{n=1,\ldots,N: x_n \neq x'_n \}.
\]
We call the databases adjacent, denoted $ x \approx x'$, iff $D_{H}(x, x')=1$.


\label{sec:b-differential-privacy}
\ndefn[Differential Privacy]{
	Fix $\veps \geq 0$. A randomised mechanism $ \mcM: \mcX \rightarrow \mcY$  is $\veps$-differentially private if for all adjacent datasets $ x \approx x'$ and each event $ A \subseteq \mcY$, 	$\;\Pr[\mcM(x) \in A] \leq e^\veps \Pr[\mcM(x') \in A]$.
	\label{def:b-dp-definition}
}

\cref{def:b-dp-definition} requires that if we perturb a database by a single datapoint, the output of the algorithm should not differ much, with the latter being controlled by the privacy parameter $\veps$.
A weaker definition of DP allows that the guarantee of~\cref{def:b-dp-definition} gets broken with probability $\delta$. This corresponds to the notion of \emph{($\veps, \delta$)-approximate differential privacy}, and will be the definition applied on our privacy-preserving summarization scheme in~\cref{chap:chap4}.

DP is equipped with a suite of properties that facilitate reasoning about privacy guarantees over complicated analysis tasks involving repeated access to a sensitive collection of data. In the remainder we review a fraction of them that will be relevant to our framework.

A useful fact about DP algorithms is that a data analyst cannot weaken their privacy guarantees by doing any computation on their output that does not depend on the private input itself.

\bnprop[Robustness to Post-Processing]
	Let $ \mcM: \mcX \rightarrow \mcY$ be $(\veps, \delta)$-DP and $\psi:\mcY \rightarrow \mcY'$ be any function. Then $\psi \circ \mcM: \mcX \rightarrow \mcY'$ is $(\veps, \delta)$-DP.
	\label{prop:bdp-postprocessing}
\enprop

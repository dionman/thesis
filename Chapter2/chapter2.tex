\chapter{Background Material}
\label{chap:chap2}

This chapter aims to set the context for the remainder of this thesis. Various concepts pertaining to this thesis including Bayesian inference and differential privacy are briefly introduced in the following.

\section{Probabilistic Learning at  a Glance}
\label{sec:b-bayesian-inference}

In (parametric) probabilistic learning problems we are generally given a set of observations $x = \{x_1,...,x_N\}$ and aim to find the parameters $\theta$ of a probabilistic model that \emph{are likely to} explain them. In the Bayesian paradigm we first assume a \emph{prior} distribution over the parameters $\pi_0(\theta)$, that encodes our beliefs about $\theta$ before observing any data. Once the observations are taken into account our beliefs shoud be updated in order to better describe the observed datapoints. For this purpose a \emph{likelihood} distribution $\pi(x|\theta)$ needs to be defined. The likelihood quantifies the probability of the observations under the assumed statistical model for parameters set to $\theta$. Combining the above distributions we are ready to formulate Bayes' theorem, the fundamental rule which gives the \emph{posterior} beliefs for our parameters updated in light of the observed data
\[
\pi(\theta|x) = \frac{\pi(x|\theta)\pi_0(\theta)}{\pi(x)}.
\label{eq:bbayes-rule}
\] 

A key computational challenge in~\cref{eq:bbayes-rule} is evaluating the normalizer, called \emph{marginal likelihood} (or \emph{model evidence}), which in a continuous parametric space takes the form
\[
\pi(x) = \int \pi(x|\theta) \pi_0(\theta) d\theta.
\label{eq:bmarginal-likelihood}
\]
\emph{Marginalising}, \ie~computing the integral of~\cref{eq:bmarginal-likelihood} can be done in closed form for a number of simple Bayesian models---some of which will be discussed in the remainder, including Gaussian mean inference, Bayesian and neural linear regression---where the likelihood is conjugate to the prior. However, for the vast majority of interesting probabilistic models marginalization cannot be done analytically and should be approximated instead. Approximate Bayesian inference has been an active research area for many decades. In the following section we present an overview of classic sampling-based and variational approaches allowing approximate computations of~\cref{eq:bbayes-rule}. 

\subsection{Sampling Methods}
\label{subsec:b-sampling-methods}

\subsection{Variational Inference}
\label{subsec:b-variational-inference}

\subsection{Laplace's Method}
\label{subsec:b-laplace-method}

\subsection{Exponential Family of Distributions}
\label{subsec:b-expfam}

\subsection{Divergence Measures}
\label{subsec:b-divergences}

\section{Robust Inference}
\label{sec:b-robust-inference}

\section{Differential Privacy}
\label{sec:b-differential-privacy}

\section{Representing Data}
\label{sec:b-representing-data}

\subsection{Kernels}
\label{subsec:b-kernels}

\subsection{Random Features}
\label{subsec:b-random-features}







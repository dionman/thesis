\chapter{Background Material}
\label{chap:chap2}

This chapter aims to set the context for the remainder of this thesis. Various concepts pertaining to this thesis, including Bayesian inference, exponential family distributions and differential privacy, are briefly introduced in the following.

\section{Probabilistic Learning at  a Glance}
\label{sec:b-bayesian-inference}

In (parametric) Bayesian learning settings we are generally given a set of observations $x = \{x_1,...,x_N\} \subseteq \mcX$ and aim to find a vector $\theta $ parameterising an assumed probabilistic model that \emph{is likely to} explain them. In the Bayesian paradigm we first assume a \emph{prior} distribution over the parameters $\pi_0(\theta)$, that encodes our beliefs about $\theta$ before observing any data. Once the data are taken into account, our beliefs shoud be updated accordingly, in order to better describe the observed distribution. For this purpose a \emph{likelihood} distribution $\pi(x|\theta)$ needs to be defined; the likelihood quantifies the probability of the observations under the assumed statistical model for parameters set to $\theta$. Combining the above distributions we are ready to formulate Bayes' theorem, the fundamental rule which gives the \emph{posterior} beliefs for our parameters updated in light of the observed data
\[
\pi(\theta|x) = \frac{\pi(x|\theta)\pi_0(\theta)}{\pi(x)}.
\label{eq:bbayes-rule}
\] 

A key computational challenge in~\cref{eq:bbayes-rule} is evaluating the normalizer, called \emph{marginal likelihood} (or \emph{model evidence}), which in a continuous parametric space takes the form
\[
\pi(x) = \int \pi(x|\theta) \pi_0(\theta) d\theta.
\label{eq:bmarginal-likelihood}
\]
\emph{Marginalising}, \ie~computing the integral of~\cref{eq:bmarginal-likelihood}, can be done using analytical tools for a number of simple Bayesian models---some of which will be discussed in the remainder, including Gaussian mean inference, Bayesian and neural linear regression---where the likelihood is conjugate to the prior. However, for the vast majority of interesting statistical models marginalization cannot be done in closed form and should be approximated instead. Aiming to address such cases approximate Bayesian inference has been an active research area for many decades. In the following section we present an overview of classic approaches allowing approximate computations of~\cref{eq:bbayes-rule}. For a more detailed exposure of approximate inference methods cf.~\citep{bishop06,murphy12,angelino16}.

\subsection{Laplace's Method}
\label{subsec:b-laplace-method}

Point estimates of $\theta$, obtained for example via \emph{maximum a posteriori~(MAP)} or \emph{maximum likelihood estimation~(MLE)}, are cheap to compute, as they correspond to solutions of optimization problems involving only the unnormalised rhs of~\cref{eq:bbayes-rule}---on the other hand, they cannot capture the uncertainty of our posterior beliefs. Laplace's method~\citep{mackay03} is an approximate inference scheme that makes a first step towards uncertainty awareness, offering a non-degenerate, yet inexpensive to compute, approximate posterior for $\theta$.

Let us write the posterior of~\cref{eq:bbayes-rule} in the following equivalent form
\[
\pi(\theta|x) = \frac{1}{Z} e^{-E(\theta)},
\]
where $E(\theta) \defined - \log \pi(\theta, x)$ is called the \emph{energy function}, and $ Z $ is the unknown normalization constant. Taking the Taylor series expansion of $\theta$ (up to order 2) around the mode $ {\htheta \defined \arg \underset{\theta}{\min} \;E(\theta)}$, we obtain the approximation 
$ \hpi(\theta, x) = e^{-E(\htheta)}\exp\left((\theta - \htheta)^T\Lambda (\theta - \htheta)\right)$ where  $ \Lambda \defined -\grad^2 E(\theta)\Big|_{\theta = \htheta}$.
Hence we have
\[
\pi(\theta|X) \approx \frac{1}{Z}   \hpi(\theta, x)  \propto \mcN(\htheta, \Lambda^{-1}), 
\label{eq:blaplace-approx}
\]  
\ie~the posterior can be approximated by a univariate Gaussian, where the mean corresponds to the minimum of the energy function and the covariance is the negative Hessian of the energy function evaluated on the mean. Clearly, using standard numerical optimization routines, e.g. quasi-Newton methods, we can achieve fast convergence to $\htheta$. 

\subsection{Sampling Methods}
\label{subsec:b-sampling-methods}
Markov Chain Monte Carlo~(MCMC) is the workhorse of approximate Bayesian inference. MCMC offers approximations to expectations \wrt intractable probability distributions via simulating an ergodic random walk which admits the target distribution as its stationary distribution; this property makes MCMC methods theoretically appealing, as it endows the estimators with strong \emph{asymptotic exactness} guarantees.

\subsection{Variational Inference}
\label{subsec:b-variational-inference}
The fundamental idea underpinning all variational inference~(VI) methods~\citep{jordan99,blei17} is to reduce Bayesian posterior inference into an optimization problem; hence, ideas allowing scaling up optimization can in principle be applied in VI, enabling scalable inference of approximate posteriors.

\subsection{Coresets}
\label{subsec:b-coresets}
Owing to their requirement for multiple evaluations of the data (log-)likelihood---a computation scaling at $\Omega(N)$---MCMC and VI methods are often prohibitively expensive in the large-data regime. Various stochastic schemes have been proposed to circumvent this computation, evaluating the likelihood on random data samples: despite achieving computational savings and being straightforward to implement, such schemes rarely offer guarantees on posterior approximation quality, and lack a rigorous principle over datapoints selection step, hence retaining part of the redundancy of the full data collection in the extracted samples.

Bayesian coresets~\citep{huggins16,campbell18,campbell19jmlr,campbell19neurips} pursue the idea of scaling up inference via the application of a preprocessing step where \emph{part of the data gets retained under the criterion of likelihood approximation}. In the spirit of the first coresets proposed in the filed of computational geometry, initial construction schemes for coreset-based inference~\citep{huggins16,lucic17training} utilize \emph{importance sampling} according to the datapoints sensitivity, \ie~a non-negative quantity measuring the redundancy of each of the datapoints for the statistical model of interest. Although giving theoretical gurantees for the approximation quality achieved by the coreset, importance sampling based constructions have typically two shortcomings: (\emph{i}) they rely on efficiently computable upper bounds of the sensitivity, and (\emph{ii}) they do not have a sense of a residual posterior error, hence are limited by common MC rates in approximating data likelihood, offering error $\eps = O(\frac{1}{\sqrt{M}})$ for coreset size $M$.

Reformulating coreset construction as sparse function approximation in a Hilbert space~(\emph{Hilbert coresets}), Campbell and Broderick~\citep{campbell18,campbell19jmlr} introduced alternative optimization formulations for the problem. They showed that using inner-product inducing norms can lead to faster incremental construction schemes that critically can be guided by the direction of greatest impovement. Moreover, they made use of a coarse posterior approximation and random projections to efficiently compute Hilbert norms that capture the divergence between the coreset and the true posterior, and proposed faster sparse constructions under polytope and hypersphere constraints.

In more recent work, Campbell and Beronov~\citep{campbell19neurips} casted Bayesian coresets to a problem of sparse variational inference within an exponential family, named \emph{Riemannian coresets}. Riemannian coresets raised the requirement for fixing a coarse posterior that appears in Hilbert coreset, offering full automation and improvement of approximation in KL over a larger range of summary sizes.


\section{Exponential Families}
\label{sec:b-expfam}

The exponential family~\citep{wainwright08} is a broad class of probability distributions, sharing a set of important properties that facilitate tractable inference. Exponential family members include numerous well-known distributions, such as the Poisson distribution, the Gamma distribution, and the Gaussian or normal distribution. 

\begin{ndefn}[Exponential family] \label{def:bexpfam}
A collection of densities $\pi$, with respect to a base measure $\nu$ indexed by a vector of parameters $\theta$, is an \emph{exponential family} of densities if it can be written as
\[
\pi_{\theta}(x) = h(x) \exp\left( \langle \theta, t(x) \rangle - Z(\theta) \right).
\label{eq:bexpfam}
\]
We call $t(x): \mcX \rightarrow \reals^{d}$ the \emph{sufficient statistics} of the data, $h(x)$ the \emph{base density} and 
\[
Z(\theta)\defined \log \int e^{\langle \theta, t(x) \rangle } h(x) \nu(dx)
\label{eq:b-logpartition}
\]
the \emph{log-partition function}.
\end{ndefn}

The parameters space of interest, referred to as the \emph{natural parameter space}, is the space $\Omega \subseteq \reals^d $ that contains all $\theta$ such that $Z(\theta)$ is finite. We say a family is \emph{regular} if $\Omega$ is open.

An important property of exponential family densities is that the derivatives of the log-partition function $Z$ are related to the moments of the sufficient statistics as follows.

\begin{nprop}[Derivatives of the log-partition function via expected statistics] \label{prop:bgradZ}
For a regular exponential family of densities in the form of~\cref{eq:bexpfam}, the log-partition function has derivatives of all orders on its domain $\Omega$, while for the first two derivatives hold the following
\[
\grad Z(\theta) = \EE_{\theta}[t(x)]
\label{eq:bgradZ}
\] 	
and 
\[
\grad^2Z(\theta) = \cov_{\theta}[t(x)] \defined \EE_{\theta}[t(x)t(x)^T] - \EE_{\theta}[t(x)]\EE_{\theta}[t(x)]^T.
\label{eq:bhessZ}
\]
\end{nprop}

\cref{prop:bgradZ} allows efficient approximations for the gradient and Hessian of $ Z $ using empirical estimates of the first two moments of the sufficient statistic, which we take advantage of in the variational approximation schemes to be introduced in~\cref{chap:chap4,chap:chap5}.

\section{Comparing Probability Distributions}
\label{subsec:b-divergences}
A critical component in constructing and evaluating inference algorithm is having a divergence measure, that captures informatively how similar two probabilities are. Statistical divergences are relaxations of distance measures, that are always non-negative and equal zero iff their arguments are identical---albeit without the requirements of being symmetric in their arguments, or satisfying the triangle inequality. The most commonly used divergence measure in approximate inference---which will directly serve as the objective quantifying the quality of our summaries in ~\cref{chap:chap4}---is the Kullback-Leibler~(KL) divergence, also named relative entropy~\citep{kullback51,kullback59}. For continuous densities $\pi_1(\theta)$ and $\pi_2(\theta)$, KL divergence is defined as 
\[
\kl{\pi_1(\theta)}{\pi_2(\theta)} \defined \int \pi_1(\theta) \log\frac{\pi_1(\theta)}{\pi_2(\theta)} d\theta.
\]
For two Gaussian distributions $\mcN_1(\mu_1, \Sigma_1)$ and $\mcN_1(\mu_1, \Sigma_1)$ the KL divergence is computable in closed form as follows
\[
\kl{\mcN_1}{\mcN_2} = \frac{1}{2}\left[\tr(\Sigma_2^{-1}\Sigma_1) - (\mu_1-\mu_2)^T\Sigma_2^{-1}(\mu_1-\mu_2) - M + \log\frac{|\Sigma_2|}{|\Sigma_1|}\right].
\]

%\section{Robust Inference}
%\label{sec:b-robust-inference}
When working in data setups that are likely to be contaminated by outliers, we get substantial performance improvements when enhancing our algorithms with \emph{robustness}. Relying on KL divergence cannot sufficiently address this concern. A robustified divergence, termed \bdiv{}, was instead proposed in~\citep{basu98,eguchi01}, that is able to downweight outlying data. Considering again the densities $\pi_1, \pi_2$, \bdiv{} is defined as 
\[
\betadistance{\pi_1}{\pi_2} \defined \frac{1}{\beta} \int\left(\pi_1^\beta(\theta) - \pi_2^\beta(\theta)\right)\pi_1(\theta)d\theta - \frac{1}{\beta+1}\int\left(\pi_1^{\beta+1}(\theta) - \pi_2^{\beta+1}(\theta)\right)d\theta.
\] 
One can easily show that \bdiv{} converges to KL divergence when $\beta \rightarrow 0$.

\section{Differential Privacy}
\label{sec:b-differential-privacy}
\ndefn[($\veps, \delta$)-Differential Privacy]{
	Fix $\veps \geq 0, \delta \in [0,1]$. A randomised algorithm
	$ \mcM: \mcX^N \rightarrow \mcX^M $  is  ($\veps, \delta$)-differentially private if for every pair of 
	adjacent datasets $ x \approx x'$ and all events $ A \subseteq \mcX^M$, 
	$\;\Pr[\mcM(x) \in A] \leq e^\veps \Pr[\mcM(x') \in A] + \delta$.
	\label{def:b-dp-definition}
}

\section{Representing Data}
\label{sec:b-representing-data}
Extracting a relevant feature representation is an important step in the context of statistical pattern recognition. For this purpose a feature map 
\[
\phi: \mcX \rightarrow \mcH,
\]
is sought which transforms the datapoints from the original data space $\{x_n\}_{n=1}^{N}$, $x_n \in \mcX$, into \emph{feature representations} in a Hilbert space $\{\phi(x_n)\}_{n=1}^{N}$, $\phi(x_n) \in \mcH$. Then the patterns of interest can be revealed via applications of inner products in the Hilbert space $\langle A, \phi(x) \rangle_{\mcH}$. There is an extensive literature on constructing data representations; for the purposes of this thesis, in the remainder of the section we focus on two of them: kernel methods and random projections.


\subsection{Kernels}
\label{subsec:b-kernels}

The main tool in kernel methods~\cite{scholkopf02} is the \emph{kernel function} defined below.
\begin{ndefn}[Kernel function] \label{def:bkernelfun}
	A symmetric function $k: \mcX \times \mcX \rightarrow \reals $ is a positive definite kernel function, or kernel, if for all $N>1$, $x_1, \ldots, x_n \in \reals$, and $c_1, \ldots, c_n \in \reals$ 
	\[
	\sum_{i,j=1}^{N} c_ic_j k(x_i, x_j) \geq 0.
	\]
\end{ndefn}
Every kernel is associated with a feature map $\phi$ as follows.
\begin{ndefn}[Kernel representation] \label{def:bkernelrepr}
	 A function $k: \mcX \times \mcX \rightarrow \reals$ is a kernel if and only if there exists a Hilbert space $\mcH$ and a feature map $\phi: \mcX \rightarrow \mcH$ such that for all $x, x' \in \mcX$
	 \[
	 \label{eq:bkernelrep}
	 k(x,x') = \langle \phi(x), \phi(x') \rangle_\mcH.
	 \]
	 Feature map $\phi$ endows each datapoint $x \in \mcX$ with a kernel representation $\phi(x)$.
\end{ndefn}
A kernel representation might be lacking an explicit closed form, but can always be accessed via the inner product of~\cref{eq:bkernelrep} which is the central object of interest in learning with kernels.

Examples of widely-used kernel functions include
\begin{itemize}
	\item The polynomial kernel $k(x,x') = \left(\langle x, x'\rangle + c\right)^d$.
	\item The Gaussian kernel $k(x,x') = \exp(-\gamma||x-x'||_{2}^2)$.
	\item The Sigmoid kernel $k(x,x') = \tanh(\beta\langle x, x'\rangle + c)$.
\end{itemize}

Kernel methods induce \emph{non-parameteric} representations on the data, \ie~when given a set with $ N $ datapoints of dimension $d$, kernels effectively map each datapoint to an $N$-dimensional representation. %Although allowing representation power scaling with the number of datapoints, such methods often reach prohibitive computational cost.

\subsection{Finite-dimensional Random Projections}
\label{subsec:b-random-features}

Kernel methods appeal to large-scale learning as they are non-parametric: their representation power scales with the number of datapoints, hence they can learn complex, highly non-linear structure from the data; however, their time and memory cost scales adversely with the dataset size. Random features~\citep{rahimi08} remedy poor complexity scaling issues via utilising \emph{parametric finite-dimensional} data representations. We motivate this concept via an application arising in Hilbert coreset constructions~\cite{campbell19jmlr}.

Denote by $ f(\theta) \defined \sum_{n=1}^{N} \log\pi(x_n|\theta) $ the log-likelihood function of a dataset $x \defined {(x_n)}_{n=1}^{N}$, and by $ f(\theta,w) \defined \sum_{n=1}^{N} w_n\log\pi(x_n|\theta) $ the corresponding log-likelihood of a Hilbert coreset $\{(w_n, x_n)\}_{n=1}^{N} : ||w||_0 \ll N$ constructed on the data. The quality of posterior approximation that this coreset offers can be quantified by using an $L^2$ norm on the log-likelihoods under a weighting distribution $\hpi$ that has the same support with the posterior $\pi$ 
\[
||f(\theta, w) - f(\theta)||_{\hpi,2} \defined \EE_{\hpi} \left[ (f(\theta) - f(\theta, w))^2\right],
\]
and induced inner product
\[
\label{eq:binner-prod-hc}
\langle f(x_n), f(x_m) \rangle_{\hpi, 2} \defined \EE_{\hpi}\left[f(x_n, \theta), f(x_m, \theta)\right].
\]
The weighting distribution $\hpi$ can be selected from a set of cheap posterior approximations, for example using Laplace's method, or running a few rounds of an MCMC algorithm. In the general case the norm of~\cref{eq:binner-prod-hc} is not available in closed form, hence a random projection can be used instead to approximate it according to the following steps:
\begin{enumerate}
	\item Sample $J$ values for $\theta$ from the weighting distribution $(\htheta_j)_{j=1}^{J} \distiid \hpi$.
	\item For $n=1 \ldots N$ compute a $J$-dimensional projection $ \hf(x_n) \defined \sqrt{\frac{1}{J}}[f(x_n, \htheta_1) \ldots f(x_n, \htheta_J)]$.
\end{enumerate}
In this way we get an unbiased finite-dimensional estimator of the inner products
\[
\langle f(x_n), f(x_m) \rangle_{\hpi, 2}  \approx \hf(x_n)^T \hf(x_m).
\]









\chapter{Introduction}
\label{chap:chap1}


\iffalse
\renewcommand\nomgroup[1]{%
	\item[\bfseries
	\ifstrequal{#1}{A}{Acronyms/Abbreviations}{
		\ifstrequal{#1}{B}{Roman Symbols}{%
			\ifstrequal{#1}{C}{Greek Symbols}{%
				\ifstrequal{#1}{D}{Other Symbols}{
					\ifstrequal{#1}{E}{Superscripts}{
						\ifstrequal{#1}{F}{Subscripts}{
							\ifstrequal{#1}{G}{Distributions}{
								\ifstrequal{#1}{H}{Operators}{
									\ifstrequal{#1}{I}{Mobile Data Abbreviations}{
										\ifstrequal{#1}{J}{Graphs}{
	}}}}}}}}}}]}
\fi

% Abbreviations
\nomenclature[A]{e.g.}{Exempli gratia}
\nomenclature[A]{i.e.}{Id est}
\nomenclature[A]{s.t.}{Such that}
\nomenclature[A]{w.r.t.}{With respect to}
\nomenclature[A]{\iid}{Independent and identically distributed}
%\nomenclature[A]{ELBO}{Evidence Lower Bound}
\nomenclature[A]{DP}{Differentially Private}
\nomenclature[A]{KL}{Kullback-Leibler}
\nomenclature[A]{MC}{Monte Carlo}
%\nomenclature[A]{HMC}{Hamiltonian Monte Carlo}
\nomenclature[A]{NUTS}{No-U-Turn Sampler}
\nomenclature[A]{CDF}{Cumulative Density Function}
\nomenclature[A]{CCDF}{Complementary Cumulative Density Function}
\nomenclature[A]{VI}{Variational Inference}
\nomenclature[A]{PSVI}{Pseudocoresets Sparse Variational Inference}
%\nomenclature[A]{\sparsevi}{Sparse VI}
\nomenclature[A]{RANDOM}{Random Sampling Coreset}
\nomenclature[A]{PL}{Privacy Loss}
\nomenclature[A]{RBF}{Radial Basis Function}
\nomenclature[A]{RMSE}{Root-Mean-Square Error}
\nomenclature[A]{PCA}{Principal Components Analysis}
\nomenclature[A]{rhs}{right hand side}
\nomenclature[A]{MLE}{Maximum Likelihood Estimation}
\nomenclature[A]{MAP}{Maximum A Posteriori}
\nomenclature[A]{\wrt}{with respect to}

% Roman symbols
\nomenclature[B]{$\mcD$}{Dataset}
\nomenclature[B]{$\mcH$}{Hilbert space}
\nomenclature[B]{$\mcX^N$ / $(\mcX \times \mcY)^N$}{Data space of $N$ unlabeled/labeled observations}
\nomenclature[D]{$[N]$}{$[1,\ldots,N]$}

% Greek symbols
%\nomenclature[C]{$\Theta$}{Space of model random variables}
\nomenclature[C]{$\eps$}{a random variable}
\nomenclature[C]{$\veps$}{a very small non-negative constant}
\nomenclature[C]{$O$}{Upper bound of complexity}
\nomenclature[C]{$\Theta$}{Asymptotically tight upper and lower bound of complexity}


% Others
\nomenclature[D]{\reals}{Real numbers}
\nomenclature[D]{\ints}{Integer numbers}
\nomenclature[D]{\nats}{Natural numbers}
\nomenclature[D]{$\#$}{Number of}
\nomenclature[D]{$\mathbb{P}$}{Probability}
\nomenclature[D]{$\plainxent$}{Cross-entropy}
\nomenclature[D]{$\plainkl$}{Kullback-Leibler divergence}
\nomenclature[D]{$\plainCapdb$}{$\beta$-divergence}
\nomenclature[D]{$\plaindb$}{$\beta$-cross-entropy}
\nomenclature[D]{$\sim$}{Distributed as}
\nomenclature[D]{$\propto$}{Proportional to}
\nomenclature[D]{$\langle , \rangle$}{Inner product}

% Superscripts
\nomenclature[E]{$\widetilde{\phantom{X}}$}{Computed on pseudodata}
\nomenclature[E]{$\widehat{\phantom{X}}$}{Empirical estimate}

% Subscripts

% Distributions
\nomenclature[G]{$\mathcal{N}$}{Normal distribution}
\nomenclature[G]{$\mathcal{T}$}{Student's t-distribution}
\nomenclature[G]{\distNamed{Unif}}{Uniform distribution}
%\nomenclature[G]{\distNamed{Gam}}{Gamma distribution}
%\nomenclature[G]{\distNamed{Poiss}}{Poisson distribution}
%\nomenclature[G]{\distNamed{Exp}}{Exponential distribution}
%\nomenclature[G]{\distNamed{Beta}}{Beta distribution}
%\nomenclature[G]{\distNamed{Dir}}{Dirichlet distribution}
\nomenclature[G]{\distNamed{Bern}}{Bernoulli distribution}
\nomenclature[G]{$\chisq$}{Chi-square distribution}
\nomenclature[G]{$\distUnifSubset$}{Uniform subset distribution}


% Operators
\nomenclature[H]{$\operatorname{diag}$}{Matrix diagonal}
\nomenclature[H]{$\operatorname{tr}$}{Matrix trace}
\nomenclature[H]{$\mathbb{E}$}{Expectation}
\nomenclature[H]{$\operatorname{Var}$}{Variance}
\nomenclature[H]{$\operatorname{Cov}$}{Covariance}
\nomenclature[H]{$\operatorname{Corr}$}{Correlation}

% Mobility data
\nomenclature[I]{MAC}{Media Access Control}
\nomenclature[I]{cid}{Cell tower identifier}
\nomenclature[I]{ID}{Identifier}
\nomenclature[I]{CDR}{Call Data Record}

% Graphs
\nomenclature[J]{$\mcG$, $G$, $V$, $E$}{Space of graphs, Graph, Vertices, Edges}
\nomenclature[J]{SP}{Shortest Path}
\nomenclature[J]{WL}{Weisfeiler-Lehman}
\nomenclature[J]{DK}{Deep graph kernels}


In this thesis we study a set of approaches that enable large-scale learning via data summarization. We start by explaining and motivating our point of view.

\section{Prelude}
\label{sec:prelude}

When faced with a data set too large to be processed all at once, an obvious solution is to retain only part of it.  In the large-scale setting, much of the data is redundant.

\par{Data Summarization and Differential Privacy}

\par{Data Summarization and Outliers Detection}

\section{Thesis Goals and Contributions}
\label{sec:thesis-goals}

The focus of this thesis is to develop scalable tools for data analysis on privacy-sensitive and vulnerable to contamination big data. Relying on the use of coreset-based dataset summarization methods as our fundamental framework for scalability, we adopt a two-pronged approach to tackle each of the aforementioned challenges, and propose efficient algorithms that outperform state-of-the-art solutions to the posed problems. 

In particular, the goals of this dissertation are to

\begin{enumerate}
	\item Identify threats in commonly adopted practices for releasing privacy-sensitive datasets via coarsened and anonymised representations of the data.
	\item Propose novel principled methods that can directly address real-word considerations of privacy and robustness when applying summarization for scalable inference, without altering the complexity footprint of the methods.
\end{enumerate}

The central constributions of the thesis are the following:

\begin{itemize}
	\item We analyse the anonymity of individual data in a large-scale behavioural study, and develop a \emph{reidentification attack that exploits structural patterns similarity} to link users records in the absence of identifiers in their state-space.
	\item We introduce a novel \emph{family of sparse variational distributions} that can approximate inference on massive datasets using learnable \emph{pseudodata}.
	\item We theoretically show that the use of learnable pseudodata enables more efficient summarization for high-dimensional datapoints, compared to existing coreset constructions that are constrained to use points from the original dataset.
	\item We provide an efficient black-box batch optimization scheme that can attain a good approximate posterior within the above-mentioned variational family, and use standard randomization tools to yield differentially private versions of this posterior for privacy-preserving data analysis.
	\item We review Bayesian coresets behaviour in corrupted datasets and show deficiencies of standard constructions when dealing with outliers and poisoning.
	\item Using tools from robust divergences, we propose a \emph{robustified family of sparse variational approximations} for reliable summarization in the presence of data contamination.
	\item We develop a black-box incremental optimization scheme for constructing such an approximation.
\end{itemize}

A recurring theme in our approach is to exploit inherent data redudancy in order to simultaneously achieve efficient data analysis and satisfy the objectives of privacy and robustness. Importantly, redudancy computation is adapted to the statistical model used to describe the data---allowing our methods to preserve good \emph{approximate sufficient statistics} of the full data---thus offering more efficiency compared to model-agnostic methods of summarization. For the purposes of privacy, directly adding noise over the data statistics computation addresses the protection of confidentiality, while using the sufficient statistics information (instead of the full data information) enables us to avoid adding more noise than necessary. In the robustness case, our framework identifies datapoints that deviate from our statistical assumptions and downweights their contribution to inference, removing them in this way from the extracted summary. Overall, our methods indicate that privacy and robustness are just two facets of the fundamental problem that data summarization puts forth.


\section{Thesis Organization}
\label{sec:thesis-organization}
The remainder of the dissertation is organized as follows. \cref{chap:chap2} introduces relevant background and concepts used throughout the thesis. \cref{chap:chap3} sheds light into the anonymity properties of a large-scale longitudinal mobility dataset, revealing a realistic privacy threat that survives in private structured datasets after coarsening individual behavioural records. \cref{chap:chap4} presents a general-purpose sparse variational inference algorithm that allows scaling up Bayesian inference in big and high-dimensional datasets via a coreset representation that relies on learnable synthetic datapoints, and introduces a differentially private construction for this coreset:~\psvi~and ~\dpsvi~respectively. \cref{chap:chap5} proposes a sparse variational approximation for robust pseudo-Bayesian posteriors using \bdiv, that can yield reliable summarizations for large-scale datasets in the presence of extensive contamination:~\bcores. Finally, \cref{chap:chap6} concludes the thesis by summarizing our results and discussing future research directions.


This thesis covers material from the following papers:

\begin{quote}
	\fullcite{manousakas2018quantifying}~(\cref{chap:chap3})
	
	\fullcite{psvi}~(\cref{chap:chap4})
	
	\fullcite{beta-cores}~(\cref{chap:chap5})
\end{quote}


In addition, the following paper was written during my PhD but is not discussed in this thesis.

\begin{quote}
	\fullcite{countering}
\end{quote}


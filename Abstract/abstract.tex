% ************************** Thesis Abstract *****************************
% Use `abstract' as an option in the document class to print only the titlepage and the abstract.
\begin{abstract}
	\label{sec:abstract} 
	The advent of large-scale datasets has offered unprecedented amounts of information for building statistically powerful machines, but, at the same time, also introduced a remarkable computational challenge: how can we efficiently process massive data? This thesis presents a suite of data reduction methods that make learning algorithms scale on large datasets, via extracting a succinct model-specific representation that summarizes the full data collection---a \emph{coreset}. Our frameworks support by design datasets of arbitrary dimensionality, and can be used for general purpose Bayesian inference under real-world constraints, including privacy preservation and robustness to outliers, encompassing diverse uncertainty-aware data analysis tasks, such as density estimation, classification and regression.
	%proliferation, abundance, gathering, acquisition \\ posed \\ trully intelligent 
   
    We motivate the necessity for novel data reduction techniques in the first place by developing a reidentification attack on coarsened representations of private behavioural data. Analysing longitudinal records of human mobility, we detect privacy-revealing structural patterns, that remain preserved in reduced graph representations of individuals information with manageable size. These unique patterns enable mounting linkage attacks on individuals information via structural similarity computations on longitudinal mobility traces, revealing an overlooked, yet existing, privacy threat.
    
    We then propose a sparse variational inference scheme for approximating posteriors on large datasets via learnable weighted \emph{pseudodata}, termed pseudocoresets. We show that the use of pseudodata enables overcoming the constraints on minimum summary size for given approximation quality, that are imposed on all existing Bayesian coreset constructions due to data dimensionality. Moreover, it allows us to develop a scheme for pseudocoresets-based summarization that satisfies the standard framework of differential privacy by construction; in this way, we can release reduced size privacy-preserving representations for sensitive datasets that are amenable to arbitrary post-processing.
    
    Subsequently, we consider summarizations for large-scale Bayesian inference in \mbox{scenarios when} observed datapoints depart from the  statistical assumptions of our model. Using robust divergences, we develop a method for constructing cleansed coresets. Crucially, this method is able to automatically remove outliers from the generated data summaries. Thus we deliver robustified scalable representations for inference, that are suitable for applications involving contaminated and unreliable data sources.
    
    We demonstrate the performance of proposed summarization techniques on multiple parametric statistical models, and diverse simulated and real-world datasets, from music genre features to hospital readmission records, considering a wide range of data dimensionalities.%, from 2 to 500.
\end{abstract}
